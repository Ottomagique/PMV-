# =============================================================================
# PARTIE 1 : BASE + AUTHENTIFICATION
# Application IPMVP Am√©lior√©e - Version 2.1 - Visualisations enrichies
# =============================================================================

import streamlit as st
import pandas as pd
import numpy as np
import io
import matplotlib.pyplot as plt
from itertools import combinations
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.model_selection import train_test_split
import math
import hashlib
import pickle
import os
from datetime import datetime, timedelta
import base64
import scipy.stats as stats
import warnings
warnings.filterwarnings('ignore')

# üìå Configuration de la page
st.set_page_config(
    page_title="Analyse IPMVP Am√©lior√©e",
    page_icon="üìä",
    layout="wide"
)

#####################################
# SYST√àME D'AUTHENTIFICATION - D√âBUT
#####################################

# Configuration de la gestion des utilisateurs
USER_DB_FILE = 'users_db.pkl'
ADMIN_USERNAME = 'admin'
ADMIN_PASSWORD = 'admin'

def hash_password(password):
    """Hache les mots de passe pour la s√©curit√©"""
    return hashlib.sha256(password.encode()).hexdigest()

def init_user_db():
    """Initialise la base de donn√©es des utilisateurs"""
    if not os.path.exists(USER_DB_FILE):
        users = {
            ADMIN_USERNAME: {
                'password': hash_password(ADMIN_PASSWORD),
                'full_name': 'Administrateur',
                'email': 'admin@example.com',
                'created_at': datetime.now(),
                'is_admin': True
            }
        }
        with open(USER_DB_FILE, 'wb') as f:
            pickle.dump(users, f)
        return users
    else:
        with open(USER_DB_FILE, 'rb') as f:
            return pickle.load(f)

def save_user_db(users):
    """Sauvegarde la base de donn√©es des utilisateurs"""
    with open(USER_DB_FILE, 'wb') as f:
        pickle.dump(users, f)

def update_user(username, password=None, full_name=None, email=None, is_admin=False):
    """Ajoute ou modifie un utilisateur"""
    users = init_user_db()
    
    if username in users:
        if password:
            users[username]['password'] = hash_password(password)
        if full_name:
            users[username]['full_name'] = full_name
        if email:
            users[username]['email'] = email
        users[username]['is_admin'] = is_admin
    else:
        users[username] = {
            'password': hash_password(password) if password else '',
            'full_name': full_name or username,
            'email': email or '',
            'created_at': datetime.now(),
            'is_admin': is_admin
        }
    
    save_user_db(users)
    return True

def delete_user(username):
    """Supprime un utilisateur (sauf admin)"""
    users = init_user_db()
    if username in users and username != ADMIN_USERNAME:
        del users[username]
        save_user_db(users)
        return True
    return False

def check_credentials(username, password):
    """V√©rifie les identifiants de connexion"""
    users = init_user_db()
    if username in users and users[username]['password'] == hash_password(password):
        return True
    return False

def is_admin(username):
    """V√©rifie si un utilisateur est administrateur"""
    users = init_user_db()
    return username in users and users[username]['is_admin']

def show_login_form():
    """Affiche le formulaire de connexion"""
    # Interface de connexion avec style moderne
    st.markdown("""
    <style>
    /* Styles de connexion */
    .login-background {
        position: fixed;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        z-index: -2;
    }
    
    .login-overlay {
        position: fixed;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        background: rgba(0, 0, 0, 0.3);
        z-index: -1;
    }
    
    .glass-panel {
        background: rgba(255, 255, 255, 0.95);
        backdrop-filter: blur(10px);
        border-radius: 20px;
        padding: 40px;
        margin: 50px auto;
        max-width: 400px;
        box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);
        border: 1px solid rgba(255, 255, 255, 0.3);
    }
    
    .brand-logo {
        text-align: center;
        margin-bottom: 30px;
    }
    
    .login-title {
        text-align: center;
        color: #00485F;
        font-size: 2.5em;
        font-weight: 800;
        margin-bottom: 10px;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
    }
    
    .login-subtitle {
        text-align: center;
        color: #666;
        margin-bottom: 30px;
        font-style: italic;
    }
    
    .login-label {
        font-weight: 600;
        color: #00485F;
        margin-bottom: 5px;
        display: block;
    }
    
    .glass-footer {
        text-align: center;
        margin-top: 30px;
        color: white;
        text-shadow: 1px 1px 2px rgba(0,0,0,0.5);
    }
    </style>
    
    <!-- Interface de connexion -->
    <div class="login-background"></div>
    <div class="login-overlay"></div>
    
    <div class="glass-panel">
        <div class="brand-logo">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 120 60" width="120">
                <rect x="20" y="15" width="80" height="30" rx="5" fill="rgba(255,255,255,0.9)"/>
                <text x="60" y="37" font-family="Arial" font-size="20" font-weight="bold" text-anchor="middle" fill="#00485F">IPMVP</text>
                <path d="M30 15 L30 45" stroke="#96B91D" stroke-width="3"/>
                <path d="M90 15 L90 45" stroke="#6DBABC" stroke-width="3"/>
            </svg>
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown('<h1 class="login-title">CALCUL & ANALYSE</h1>', unsafe_allow_html=True)
    st.markdown('<p class="login-subtitle">Outil d\'analyse et de mod√©lisation √©nerg√©tique conforme aux standards IPMVP</p>', unsafe_allow_html=True)
    
    # Gestion de l'√©tat de connexion
    if "login_status" not in st.session_state:
        st.session_state.login_status = None
    
    if st.session_state.login_status == "failed":
        st.error("Identifiants incorrects. Veuillez r√©essayer.")
    
    # Formulaire de connexion
    with st.form("login_form"):
        st.markdown('<label for="username" class="login-label">Nom d\'utilisateur</label>', unsafe_allow_html=True)
        username = st.text_input("", key="username_input", label_visibility="collapsed")
        st.markdown('<label for="password" class="login-label">Mot de passe</label>', unsafe_allow_html=True)
        password = st.text_input("", type="password", key="password_input", label_visibility="collapsed")
        
        submitted = st.form_submit_button("Se connecter")
        
        if submitted:
            if check_credentials(username, password):
                st.session_state.login_successful = True
                st.session_state.logged_username = username
                st.session_state.logged_admin = is_admin(username)
            else:
                st.session_state.login_status = "failed"
                st.session_state.login_successful = False
    
    # Pied de page
    st.markdown("""
    <div class="glass-footer">
        <p>D√©velopp√© avec ‚ù§Ô∏è par <strong>Efficacit√© Energ√©tique, Carbone & RSE team</strong> ¬© 2025</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Mise √† jour de l'√©tat apr√®s rendu
    if st.session_state.get('login_successful', False):
        st.session_state['authenticated'] = True
        st.session_state['username'] = st.session_state.logged_username
        st.session_state['is_admin'] = st.session_state.logged_admin
        
        # Nettoyage des variables temporaires
        del st.session_state['login_successful']
        del st.session_state['logged_username']
        del st.session_state['logged_admin']
        del st.session_state['login_status']
        
        st.rerun()

def show_admin_panel():
    """Interface d'administration des utilisateurs"""
    st.header("üîê Administration des utilisateurs")
    
    users = init_user_db()
    
    # Liste des utilisateurs existants
    st.subheader("üë• Utilisateurs existants")
    
    user_data = []
    for username, data in users.items():
        user_data.append({
            "Nom d'utilisateur": username,
            "Nom complet": data.get('full_name', ''),
            "Email": data.get('email', ''),
            "Date de cr√©ation": data.get('created_at', '').strftime('%d/%m/%Y') if 'created_at' in data else '',
            "Admin": "‚úÖ" if data.get('is_admin', False) else "‚ùå"
        })
    
    st.table(user_data)
    
    # Onglets pour la gestion
    tab1, tab2 = st.tabs(["‚ûï Ajouter/Modifier", "üóëÔ∏è Supprimer"])
    
    with tab1:
        with st.form("user_form"):
            col1, col2 = st.columns(2)
            with col1:
                username = st.text_input("Nom d'utilisateur*")
            with col2:
                password = st.text_input("Mot de passe*", type="password")
            
            col1, col2 = st.columns(2)
            with col1:
                full_name = st.text_input("Nom complet")
            with col2:
                email = st.text_input("Email")
            
            is_admin_checkbox = st.checkbox("Administrateur")
            
            submit = st.form_submit_button("üíæ Enregistrer l'utilisateur", use_container_width=True)
            
            if submit:
                if not username or not password:
                    st.error("‚ùå Le nom d'utilisateur et le mot de passe sont obligatoires.")
                else:
                    update_user(username, password, full_name, email, is_admin_checkbox)
                    st.success(f"‚úÖ Utilisateur '{username}' enregistr√© avec succ√®s.")
                    st.rerun()
    
    with tab2:
        with st.form("delete_user_form"):
            user_to_delete = st.selectbox(
                "S√©lectionner un utilisateur √† supprimer",
                [u for u in users.keys() if u != ADMIN_USERNAME]
            )
            
            delete_submit = st.form_submit_button("üóëÔ∏è Supprimer l'utilisateur", type="primary", use_container_width=True)
            
            if delete_submit:
                if delete_user(user_to_delete):
                    st.success(f"‚úÖ Utilisateur '{user_to_delete}' supprim√© avec succ√®s.")
                    st.rerun()
                else:
                    st.error("‚ùå Impossible de supprimer cet utilisateur.")
    
    # Retour √† l'application
    if st.button("üîô Retour √† l'application", use_container_width=True):
        st.session_state['show_admin'] = False
        st.rerun()

def show_navbar():
    """Barre de navigation avec informations utilisateur"""
    col1, col2, col3 = st.columns([2, 1, 1])
    
    with col1:
        st.markdown(f"<div style='padding: 10px 0;'>üë§ Connect√© en tant que: <b>{st.session_state['username']}</b></div>", unsafe_allow_html=True)
    
    with col2:
        if st.session_state.get('is_admin', False):
            if st.button("üîê Administration", key="admin_button", use_container_width=True):
                st.session_state['show_admin'] = not st.session_state.get('show_admin', False)
                st.rerun()
    
    with col3:
        if st.button("üö™ D√©connexion", key="logout_button", use_container_width=True):
            for key in list(st.session_state.keys()):
                del st.session_state[key]
            st.rerun()

# Initialisation des variables de session
if 'authenticated' not in st.session_state:
    st.session_state['authenticated'] = False
if 'show_admin' not in st.session_state:
    st.session_state['show_admin'] = False

# V√©rification de l'authentification
if not st.session_state['authenticated']:
    show_login_form()
    st.stop()

# Affichage du panneau d'administration si demand√©
if st.session_state.get('show_admin', False) and st.session_state.get('is_admin', False):
    show_admin_panel()
    st.stop()

###################################
# SYST√àME D'AUTHENTIFICATION - FIN
###################################
# =============================================================================
# PARTIE 2 : FONCTIONS M√âTIER IPMVP
# Fonctions de calcul statistique, validation et scoring am√©lior√©es
# =============================================================================

# NOUVELLES FONCTIONS POUR L'ANALYSE IPMVP AM√âLIOR√âE

def detecter_colonnes(df):
    """
    D√©tecte automatiquement les colonnes de date et de consommation
    """
    date_col_guess = None
    conso_col_guess = None
    
    if df is None or df.empty:
        return date_col_guess, conso_col_guess
    
    # 1. D√©tecter la colonne de date
    date_keywords = ['date', 'temps', 'p√©riode', 'period', 'time', 'jour', 'day', 'mois', 'month', 'ann√©e', 'year']
    
    # Essayer d'abord les colonnes datetime
    datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()
    if datetime_cols:
        date_col_guess = datetime_cols[0]
    else:
        # Chercher par mots-cl√©s
        for keyword in date_keywords:
            potential_cols = [col for col in df.columns if keyword.lower() in col.lower()]
            if potential_cols:
                for col in potential_cols:
                    try:
                        pd.to_datetime(df[col])
                        date_col_guess = col
                        break
                    except:
                        continue
                if date_col_guess:
                    break
    
    # 2. D√©tecter la colonne de consommation
    conso_keywords = ['consommation', 'conso', '√©nergie', 'energy', 'kwh', 'mwh', 'wh', 
                      '√©lectricit√©', 'electricity', 'gaz', 'gas', 'chaleur', 'heat', 
                      'puissance', 'power', 'compteur', 'meter']
    
    cols_to_check = [col for col in df.columns if col != date_col_guess]
    
    # Chercher par mots-cl√©s
    for keyword in conso_keywords:
        potential_cols = [col for col in cols_to_check if keyword.lower() in col.lower()]
        if potential_cols:
            for col in potential_cols:
                try:
                    if pd.to_numeric(df[col], errors='coerce').notna().sum() > 0.8 * len(df):
                        conso_col_guess = col
                        break
                except:
                    continue
            if conso_col_guess:
                break
    
    # Si aucune correspondance, chercher une colonne num√©rique
    if not conso_col_guess:
        numeric_cols = [col for col in cols_to_check if 
                        pd.api.types.is_numeric_dtype(df[col]) or 
                        pd.to_numeric(df[col], errors='coerce').notna().sum() > 0.8 * len(df)]
        if numeric_cols:
            for col in numeric_cols:
                if not (col.lower().startswith('id') or col.lower().startswith('index')):
                    conso_col_guess = col
                    break
            if not conso_col_guess and numeric_cols:
                conso_col_guess = numeric_cols[0]
    
    return date_col_guess, conso_col_guess

def calculate_t_stats(X, y, model, coefs):
    """
    Calcule les valeurs t-stat pour les coefficients de r√©gression
    """
    if not hasattr(model, 'coef_'):
        return {feature: None for feature in coefs.keys()}
    
    try:
        # Calcul des pr√©dictions et r√©sidus
        y_pred = model.predict(X)
        residuals = y - y_pred
        
        # Degr√©s de libert√© et MSE
        n = len(y)
        p = len(model.coef_)
        df = n - p - 1
        if df <= 0:
            return {feature: None for feature in coefs.keys()}
            
        mse = np.sum(residuals ** 2) / df
        
        # Calcul de la matrice (X'X)^-1
        X_matrix = X.values
        XtX_inv = np.linalg.inv(np.dot(X_matrix.T, X_matrix))
        
        # Erreurs standard
        se = np.sqrt(np.diag(XtX_inv) * mse)
        
        # Calcul des valeurs t
        t_stats = model.coef_ / se
        
        # Calcul des p-values
        p_values = [2 * (1 - stats.t.cdf(abs(t), df)) for t in t_stats]
        
        # Cr√©er un dictionnaire des r√©sultats
        result = {}
        for i, feature in enumerate(X.columns):
            result[feature] = {
                't_value': t_stats[i],
                'p_value': p_values[i],
                'significant': p_values[i] < 0.05
            }
        
        return result
    except:
        return {feature: None for feature in X.columns}

def detect_overfitting_intelligent(model_info, nb_observations):
    """
    D√©tection intelligente de l'overfitting selon le contexte
    """
    r2 = model_info['r2']
    nb_variables = len(model_info['features'])
    model_type = model_info['model_type']
    
    # Calcul du ratio observations/variables
    ratio = nb_observations / nb_variables if nb_variables > 0 else float('inf')
    
    # Crit√®res d'overfitting adaptatifs
    is_overfitted = False
    warning_msg = ""
    severity = "info"
    
    # 1. R¬≤ extr√™me (toujours suspect)
    if r2 > 0.995:
        is_overfitted = True
        warning_msg = "üö® R¬≤ extr√™me (>99.5%) - Overfitting quasi certain"
        severity = "error"
    
    # 2. R¬≤ tr√®s √©lev√© avec contexte dangereux
    elif r2 > 0.98:
        if ratio < 5:  # Moins de 5 observations par variable
            is_overfitted = True
            warning_msg = f"üö® R¬≤ = {r2:.3f} avec ratio obs/var = {ratio:.1f} - Overfitting probable"
            severity = "error"
        elif model_type == "Polynomiale":
            is_overfitted = True
            warning_msg = f"‚ö†Ô∏è Mod√®le polynomial avec R¬≤ = {r2:.3f} - Risque overfitting √©lev√©"
            severity = "warning"
        elif nb_variables > 3:
            warning_msg = f"‚ö†Ô∏è R¬≤ = {r2:.3f} avec {nb_variables} variables - V√©rifier la robustesse"
            severity = "warning"
    
    # 3. Ratio dangereux m√™me avec R¬≤ mod√©r√©
    elif ratio < 3:
        is_overfitted = True
        warning_msg = f"üö® Ratio observations/variables = {ratio:.1f} - Donn√©es insuffisantes"
        severity = "error"
    elif ratio < 5:
        warning_msg = f"‚ö†Ô∏è Ratio observations/variables = {ratio:.1f} - Risque overfitting"
        severity = "warning"
    
    return is_overfitted, warning_msg, severity

def calculate_ipmvp_score(model_info, nb_observations):
    """
    Calcule un score composite IPMVP de 0 √† 100 points
    """
    r2 = model_info['r2']
    cv_rmse = model_info['cv_rmse']
    bias = abs(model_info['bias'])
    nb_variables = len(model_info['features'])
    model_type = model_info['model_type']
    
    # Score de base (60 points max)
    # R¬≤ : 30 points max
    r2_score = min(r2 / 0.75, 1.0) * 30 if r2 >= 0.5 else r2 * 20
    
    # CV(RMSE) : 20 points max (invers√© - plus faible = mieux)
    cv_score = max(0, min((0.25 - cv_rmse) / 0.25, 1.0)) * 20
    
    # Biais : 10 points max
    bias_score = max(0, min((10 - bias) / 10, 1.0)) * 10
    
    base_score = r2_score + cv_score + bias_score
    
    # Bonus/Malus (40 points max)
    bonus_malus = 0
    
    # Bonus simplicit√© (15 points max)
    if nb_variables == 1:
        bonus_malus += 15
    elif nb_variables == 2:
        bonus_malus += 10
    elif nb_variables == 3:
        bonus_malus += 5
    
    # Bonus conformit√© IPMVP (15 points max)
    if r2 >= 0.75 and cv_rmse <= 0.15 and bias <= 5:
        bonus_malus += 15
    elif r2 >= 0.6 and cv_rmse <= 0.2 and bias <= 8:
        bonus_malus += 10
    elif r2 >= 0.5 and cv_rmse <= 0.25:
        bonus_malus += 5
    
    # Bonus significativit√© statistique (10 points max)
    if 't_stats' in model_info and model_type in ["Lin√©aire", "Ridge", "Lasso"]:
        significant_vars = 0
        total_vars = 0
        for feature in model_info['features']:
            if (feature in model_info['t_stats'] and 
                model_info['t_stats'][feature] is not None):
                total_vars += 1
                t_val = model_info['t_stats'][feature]
                if isinstance(t_val, dict) and 't_value' in t_val:
                    if abs(t_val['t_value']) > 2:
                        significant_vars += 1
                elif isinstance(t_val, (int, float)) and abs(t_val) > 2:
                    significant_vars += 1
        
        if total_vars > 0:
            sig_ratio = significant_vars / total_vars
            bonus_malus += sig_ratio * 10
    
    # Malus overfitting
    is_overfitted, _, severity = detect_overfitting_intelligent(model_info, nb_observations)
    if is_overfitted:
        if severity == "error":
            bonus_malus -= 30  # Gros malus
        else:
            bonus_malus -= 15  # Malus mod√©r√©
    
    # Malus mod√®le complexe
    if model_type == "Polynomiale":
        bonus_malus -= 5
    
    # Score final (0-100)
    final_score = max(0, min(100, base_score + bonus_malus))
    
    return final_score

def validate_data_quality(df, date_col, conso_col, selected_vars):
    """
    Valide la qualit√© des donn√©es avant l'analyse
    """
    issues = []
    warnings = []
    
    # 1. V√©rification des donn√©es manquantes
    missing_dates = df[date_col].isnull().sum()
    missing_conso = df[conso_col].isnull().sum()
    
    if missing_dates > 0:
        issues.append(f"‚ùå {missing_dates} dates manquantes d√©tect√©es")
    
    if missing_conso > 0:
        issues.append(f"‚ùå {missing_conso} valeurs de consommation manquantes")
    
    # 2. V√©rification des variables explicatives
    for var in selected_vars:
        missing_var = df[var].isnull().sum()
        if missing_var > len(df) * 0.1:  # Plus de 10% manquant
            warnings.append(f"‚ö†Ô∏è Variable '{var}': {missing_var} valeurs manquantes ({missing_var/len(df)*100:.1f}%)")
    
    # 3. V√©rification de la r√©gularit√© temporelle
    if pd.api.types.is_datetime64_any_dtype(df[date_col]):
        date_diff = df[date_col].diff().dropna()
        if date_diff.std().days > 5:  # Irr√©gularit√© > 5 jours
            warnings.append("‚ö†Ô∏è Espacement irr√©gulier entre les dates d√©tect√©")
    
    # 4. V√©rification des valeurs aberrantes (consommation)
    if pd.api.types.is_numeric_dtype(df[conso_col]):
        q1 = df[conso_col].quantile(0.25)
        q3 = df[conso_col].quantile(0.75)
        iqr = q3 - q1
        outliers = ((df[conso_col] < q1 - 1.5*iqr) | (df[conso_col] > q3 + 1.5*iqr)).sum()
        if outliers > len(df) * 0.05:  # Plus de 5% d'outliers
            warnings.append(f"‚ö†Ô∏è {outliers} valeurs aberrantes potentielles dans la consommation ({outliers/len(df)*100:.1f}%)")
    
    # 5. V√©rification du nombre minimum de donn√©es
    if len(df) < 12:
        issues.append(f"‚ùå Donn√©es insuffisantes: {len(df)} points (minimum 12 requis)")
    elif len(df) < 24:
        warnings.append(f"‚ö†Ô∏è Donn√©es limit√©es: {len(df)} points (24+ recommand√©s pour train/test)")
    
    return issues, warnings

def check_variable_limits(nb_observations, nb_variables, model_type):
    """
    V√©rifie les limitations de s√©curit√© pour √©viter l'overfitting
    """
    issues = []
    warnings = []
    
    # R√®gle des 10:1 pour les observations/variables
    max_vars_recommended = nb_observations // 10
    max_vars_minimum = nb_observations // 5  # Seuil critique
    
    if nb_variables > max_vars_minimum:
        issues.append(f"üö® Trop de variables: {nb_variables} avec {nb_observations} observations (ratio {nb_observations/nb_variables:.1f}:1)")
        issues.append(f"Maximum critique: {max_vars_minimum} variables")
    elif nb_variables > max_vars_recommended:
        warnings.append(f"‚ö†Ô∏è Ratio observations/variables: {nb_observations/nb_variables:.1f}:1 (recommand√©: ‚â•10:1)")
        warnings.append(f"Recommandation: maximum {max_vars_recommended} variables")
    
    # Limitations sp√©cifiques aux mod√®les polynomiaux
    if model_type == "Polynomiale":
        if nb_observations < 20:
            issues.append(f"üö® Mod√®le polynomial n√©cessite ‚â•20 observations (actuellement: {nb_observations})")
        elif nb_observations < 30:
            warnings.append(f"‚ö†Ô∏è Mod√®le polynomial avec {nb_observations} observations - Risque d'instabilit√©")
        
        # Estimation du nombre de param√®tres g√©n√©r√©s
        estimated_params = nb_variables * 2 + nb_variables  # Approximation pour degr√© 2
        if estimated_params > nb_observations // 3:
            warnings.append(f"‚ö†Ô∏è Mod√®le polynomial g√©n√©rera ~{estimated_params} param√®tres - Risque de complexit√© excessive")
    
    return issues, warnings

def format_equation(intercept, coefficients, threshold=1e-4):
    """
    Formate l'√©quation du mod√®le en ignorant les coefficients n√©gligeables
    """
    equation = f"Consommation = {intercept:.4f}"
    
    # Trier les coefficients par valeur absolue d√©croissante
    sorted_coefs = sorted(coefficients.items(), key=lambda x: abs(x[1]), reverse=True)
    
    for feature, coef in sorted_coefs:
        # Ignorer les coefficients proches de z√©ro
        if abs(coef) < threshold:
            continue
            
        sign = "+" if coef >= 0 else ""
        equation += f" {sign} {coef:.4f} √ó {feature}"
    
    return equation

def tooltip(text, explanation):
    """
    Cr√©e une info-bulle explicative
    """
    return f'<span>{text} <span class="tooltip">‚ÑπÔ∏è<span class="tooltiptext tooltip-right">{explanation}</span></span></span>'

def evaluer_conformite(r2, cv_rmse, bias=None):
    """
    √âvalue la conformit√© IPMVP avec crit√®res enrichis
    """
    # Crit√®res principaux
    r2_ok = r2 >= 0.75
    cv_ok = cv_rmse <= 0.15
    bias_ok = bias is None or abs(bias) <= 5
    
    if r2_ok and cv_ok and bias_ok:
        return "Excellente", "good"
    elif r2 >= 0.6 and cv_rmse <= 0.2 and (bias is None or abs(bias) <= 8):
        return "Bonne", "medium"
    elif r2 >= 0.5 and cv_rmse <= 0.25:
        return "Acceptable", "medium"
    else:
        return "Insuffisante", "bad"

def format_value(value, fmt=".4f", default="N/A"):
    """
    Formate une valeur num√©rique de mani√®re s√©curis√©e
    """
    if value is None:
        return default
    
    try:
        if isinstance(value, (int, float)):
            return f"{value:{fmt}}"
        elif isinstance(value, dict) and 't_value' in value and isinstance(value['t_value'], (int, float)):
            return f"{value['t_value']:{fmt}}"
        return default
    except:
        return default

def should_use_train_test_split(nb_observations):
    """
    D√©termine si on doit utiliser un split train/test
    """
    if nb_observations >= 24:
        return True, "üöÄ Mode validation robuste: Split train/test activ√©"
    elif nb_observations >= 18:
        return False, f"‚ö†Ô∏è {nb_observations} mois disponibles - Split train/test recommand√© avec ‚â•24 mois"
    else:
        return False, f"üìã Mode IPMVP standard avec {nb_observations} mois de donn√©es"

def create_train_test_split(df, date_col, train_months=18):
    """
    Cr√©e un split train/test temporel pour les donn√©es IPMVP
    """
    # Trier par date
    df_sorted = df.sort_values(by=date_col)
    
    # Calculer le point de coupure (18 premiers mois pour train)
    min_date = df_sorted[date_col].min()
    split_date = min_date + pd.DateOffset(months=train_months)
    
    # Split des donn√©es
    train_df = df_sorted[df_sorted[date_col] < split_date]
    test_df = df_sorted[df_sorted[date_col] >= split_date]
    
    return train_df, test_df, split_date
# =============================================================================
# PARTIE 3 : INTERFACE UTILISATEUR
# CSS, styling, sidebar et interface principale avec contr√¥les adaptatifs
# =============================================================================

# CSS AM√âLIOR√â AVEC NOUVEAUX STYLES POUR LES AM√âLIORATIONS IPMVP
st.markdown("""
    <style>
    @import url('https://fonts.googleapis.com/css2?family=Manrope:wght@400;700;800&display=swap');

    html, body, [class*="st-"] {
        font-family: 'Manrope', sans-serif;
        color: #0C1D2D;
    }

    h1, h2, h3 {
        font-weight: 800;
        color: #00485F;
    }

    h4, h5, h6 {
        font-weight: 700;
        color: #00485F;
    }

    .stButton>button {
        background-color: #6DBABC;
        color: white;
        border-radius: 8px;
        padding: 12px 18px;
        font-size: 16px;
        font-weight: bold;
        border: none;
        transition: all 0.3s ease-in-out;
    }

    .stButton>button:hover {
        background-color: #96B91D;
        color: white;
        transform: scale(1.05);
    }

    .stSidebar {
        background-color: #E7DDD9;
        padding: 20px;
        border-radius: 10px;
    }

    input, select, textarea {
        background-color: #E7DDD9 !important;
        border-radius: 5px;
        border: 1px solid #00485F;
    }

    .block-container {
        padding: 2rem;
        border-radius: 10px;
        background-color: #E7DDD9;
    }

    .stDataFrame {
        border: 1px solid #0C1D2D;
        border-radius: 10px;
    }

    .metrics-card {
        background-color: #E7DDD9;
        border-radius: 10px;
        padding: 15px;
        margin: 10px 0;
        box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        border: 1px solid #00485F;
    }
    
    .equation-box {
        background-color: #E7DDD9;
        border-left: 4px solid #6DBABC;
        padding: 15px;
        margin: 15px 0;
        border-radius: 0 10px 10px 0;
        font-family: monospace;
        border: 1px solid #00485F;
    }
    
    .conformity-good {
        color: #96B91D;
        font-weight: bold;
    }
    
    .conformity-medium {
        color: #f39c12;
        font-weight: bold;
    }
    
    .conformity-bad {
        color: #e74c3c;
        font-weight: bold;
    }
    
    .footer-credit {
        text-align: center;
        margin-top: 30px;
        padding: 15px;
        background-color: #00485F;
        color: white;
        border-radius: 10px;
        font-size: 14px;
    }
    
    .instruction-card {
        background-color: #E7DDD9;
        border-left: 4px solid #96B91D;
        padding: 15px;
        margin: 15px 0;
        border-radius: 0 10px 10px 0;
    }
    
    .table-header {
        background-color: #00485F;
        color: white;
    }
    
    /* Styles pour les info-bulles */
    .tooltip {
        position: relative;
        display: inline-block;
        cursor: help;
        color: #00485F;
        font-size: 14px;
        margin-left: 4px;
    }
    
    .tooltip .tooltiptext {
        visibility: hidden;
        width: 250px;
        background-color: #00485F;
        color: white;
        text-align: left;
        border-radius: 6px;
        padding: 10px;
        position: absolute;
        z-index: 1;
        bottom: 125%;
        left: 50%;
        margin-left: -125px;
        opacity: 0;
        transition: opacity 0.3s;
        font-size: 13px;
        line-height: 1.4;
        box-shadow: 0 3px 10px rgba(0,0,0,0.2);
    }
    
    .tooltip .tooltiptext::after {
        content: "";
        position: absolute;
        top: 100%;
        left: 50%;
        margin-left: -5px;
        border-width: 5px;
        border-style: solid;
        border-color: #00485F transparent transparent transparent;
    }
    
    .tooltip:hover .tooltiptext {
        visibility: visible;
        opacity: 1;
    }
    
    .tooltip-right {
        left: 100% !important;
        margin-left: 10px !important;
        bottom: 0 !important;
    }

    .tooltip-right::after {
        top: 50% !important;
        left: -5px !important;
        margin-left: 0 !important;
        margin-top: -5px !important;
        border-width: 5px !important;
        border-style: solid !important;
        border-color: transparent #00485F transparent transparent !important;
    }
    
    .model-badge {
        display: inline-block;
        background-color: #6DBABC;
        color: white;
        padding: 4px 8px;
        border-radius: 4px;
        font-size: 12px;
        font-weight: bold;
        margin-left: 8px;
    }

    /* Styles pour les tableaux statistiques */
    .stats-table {
        width: 100%;
        border-collapse: collapse;
        margin: 15px 0;
        border-radius: 5px;
        overflow: hidden;
    }
    
    .stats-table th {
        background-color: #00485F;
        color: white;
        padding: 8px 12px;
        text-align: left;
        font-weight: 600;
    }
    
    .stats-table td {
        padding: 8px 12px;
        border-bottom: 1px solid #e0e0e0;
    }
    
    .stats-table tr:nth-child(even) {
        background-color: rgba(109, 186, 188, 0.1);
    }
    
    .stats-table tr:hover {
        background-color: rgba(150, 185, 29, 0.1);
    }
    
    /* Badges de significativit√© */
    .significance-badge {
        display: inline-block;
        padding: 2px 6px;
        border-radius: 3px;
        font-size: 11px;
        font-weight: bold;
    }
    
    .significant {
        background-color: #96B91D;
        color: white;
    }
    
    .not-significant {
        background-color: #e74c3c;
        color: white;
    }

    /* NOUVEAUX STYLES POUR LES AM√âLIORATIONS IPMVP */
    
    /* Alertes et warnings */
    .alert-card {
        border-radius: 8px;
        padding: 12px;
        margin: 10px 0;
        border-left: 4px solid;
    }
    
    .alert-error {
        background-color: #ffebee;
        border-color: #f44336;
        color: #c62828;
    }
    
    .alert-warning {
        background-color: #fff8e1;
        border-color: #ff9800;
        color: #f57c00;
    }
    
    .alert-info {
        background-color: #e3f2fd;
        border-color: #2196f3;
        color: #1565c0;
    }
    
    .alert-success {
        background-color: #e8f5e8;
        border-color: #4caf50;
        color: #2e7d32;
    }

    /* Scores et m√©triques am√©lior√©es */
    .score-card {
        background: linear-gradient(135deg, #6DBABC 0%, #96B91D 100%);
        border-radius: 15px;
        padding: 20px;
        color: white;
        text-align: center;
        margin: 15px 0;
        box-shadow: 0 4px 15px rgba(0,0,0,0.1);
    }
    
    .score-value {
        font-size: 2.5em;
        font-weight: 800;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
    }
    
    .score-label {
        font-size: 1.1em;
        opacity: 0.9;
        margin-top: 5px;
    }

    /* Badges de statut */
    .status-badge {
        display: inline-block;
        padding: 4px 12px;
        border-radius: 20px;
        font-size: 12px;
        font-weight: bold;
        text-transform: uppercase;
    }
    
    .status-excellent {
        background-color: #96B91D;
        color: white;
    }
    
    .status-good {
        background-color: #6DBABC;
        color: white;
    }
    
    .status-warning {
        background-color: #ff9800;
        color: white;
    }
    
    .status-error {
        background-color: #f44336;
        color: white;
    }

    /* Mode train/test */
    .mode-indicator {
        background-color: rgba(109, 186, 188, 0.1);
        border: 2px solid #6DBABC;
        border-radius: 10px;
        padding: 15px;
        margin: 15px 0;
        text-align: center;
    }
    
    .mode-title {
        font-weight: bold;
        color: #00485F;
        font-size: 1.2em;
        margin-bottom: 5px;
    }

    /* Limitations et warnings */
    .limitation-box {
        background-color: #fff3e0;
        border: 2px solid #ff9800;
        border-radius: 8px;
        padding: 15px;
        margin: 10px 0;
    }
    
    .limitation-title {
        font-weight: bold;
        color: #e65100;
        margin-bottom: 8px;
    }

    /* Comparaison train/test */
    .comparison-grid {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 15px;
        margin: 15px 0;
    }
    
    .train-card {
        background-color: rgba(150, 185, 29, 0.1);
        border-left: 4px solid #96B91D;
        padding: 15px;
        border-radius: 0 8px 8px 0;
    }
    
    .test-card {
        background-color: rgba(109, 186, 188, 0.1);
        border-left: 4px solid #6DBABC;
        padding: 15px;
        border-radius: 0 8px 8px 0;
    }

    /* Progress bar personnalis√©e */
    .progress-container {
        background-color: #e0e0e0;
        border-radius: 10px;
        overflow: hidden;
        margin: 10px 0;
    }
    
    .progress-bar {
        height: 20px;
        background: linear-gradient(90deg, #6DBABC 0%, #96B91D 100%);
        border-radius: 10px;
        transition: width 0.3s ease;
    }
    </style>
    """, unsafe_allow_html=True)

# TITRE ET DESCRIPTION PRINCIPALE
st.title("üìä Analyse IPMVP Am√©lior√©e")
st.markdown("""
Bienvenue dans **l'Analyse IPMVP Am√©lior√©e** ! üöÄ  
Cette version inclut la **d√©tection d'overfitting**, le **scoring composite**, et la **validation train/test** pour des analyses plus robustes.
""")

# GUIDE D'UTILISATION AM√âLIOR√â
st.markdown("""
<div class="instruction-card">
<h3>üõ†Ô∏è Guide d'utilisation - Version Am√©lior√©e</h3>
<h4>üìã Nouveaut√©s de cette version :</h4>
<ul>
    <li><strong>üõ°Ô∏è D√©tection d'overfitting intelligente</strong> : Rejet automatique des mod√®les avec R¬≤ artificiellement gonfl√©</li>
    <li><strong>üéØ Score composite IPMVP</strong> : S√©lection des mod√®les bas√©e sur un score 0-100 points (R¬≤ + CV(RMSE) + simplicit√© + significativit√©)</li>
    <li><strong>üöÄ Mode train/test adaptatif</strong> : Split automatique 18/6 mois si ‚â•24 mois de donn√©es</li>
    <li><strong>‚ö†Ô∏è Limitations s√©curit√©</strong> : Contr√¥le du ratio observations/variables (r√®gle 10:1)</li>
    <li><strong>üìä M√©triques enrichies</strong> : Comparaison train/test, valeurs t de Student, warnings intelligents</li>
</ul>

<h4>üîÑ Flux d'analyse intelligent :</h4>
<ol>
    <li><strong>Validation des donn√©es</strong> : V√©rification qualit√©, d√©tection anomalies</li>
    <li><strong>Mode adaptatif</strong> : 
        <ul>
            <li>‚â•24 mois ‚Üí Mode "Validation robuste" avec train/test</li>
            <li>12-23 mois ‚Üí Mode "IPMVP standard" avec protections renforc√©es</li>
        </ul>
    </li>
    <li><strong>Limitations automatiques</strong> : 
        <ul>
            <li>Variables max = nb_observations √∑ 10</li>
            <li>Polyn√¥me seulement si ‚â•20 observations</li>
        </ul>
    </li>
    <li><strong>S√©lection intelligente</strong> : Score composite privil√©giant robustesse + simplicit√©</li>
    <li><strong>R√©sultats enrichis</strong> : M√©triques avanc√©es, warnings, recommandations</li>
</ol>

<h4>‚úÖ Crit√®res de qualit√© IPMVP renforc√©s :</h4>
<ul>
    <li><strong>R¬≤ ‚â• 0.75</strong> : Corr√©lation excellente</li>
    <li><strong>CV(RMSE) ‚â§ 15%</strong> : Pr√©cision excellente</li>
    <li><strong>|Biais| < 5%</strong> : Ajustement √©quilibr√©</li>
    <li><strong>Variables significatives</strong> : |t| > 2 (p-value < 0.05)</li>
    <li><strong>Ratio obs/var ‚â• 10:1</strong> : Donn√©es suffisantes</li>
</ul>
</div>
""", unsafe_allow_html=True)

# IMPORT DE FICHIER ET BOUTON DE CALCUL
col1, col2 = st.columns([3, 1])

with col1:
    uploaded_file = st.file_uploader("üìÇ Importer un fichier Excel", type=["xlsx", "xls"])

with col2:
    lancer_calcul = st.button("üöÄ Lancer l'analyse", use_container_width=True)

# TRAITEMENT DU FICHIER AVEC VALIDATION AM√âLIOR√âE
if uploaded_file:
    try:
        df = pd.read_excel(uploaded_file)
        
        # D√©tection automatique des colonnes
        date_col_guess, conso_col_guess = detecter_colonnes(df)
        
        # Messages d'information am√©lior√©s
        if date_col_guess and conso_col_guess:
            st.success(f"‚úÖ **D√©tection automatique r√©ussie**")
            col1, col2 = st.columns(2)
            with col1:
                st.info(f"üìÖ **Date** : '{date_col_guess}'")
            with col2:
                st.info(f"‚ö° **Consommation** : '{conso_col_guess}'")
        elif date_col_guess:
            st.info(f"üìÖ Colonne de date d√©tect√©e : '{date_col_guess}'")
            st.warning("‚ö†Ô∏è Veuillez s√©lectionner manuellement la colonne de consommation.")
        elif conso_col_guess:
            st.info(f"‚ö° Colonne de consommation d√©tect√©e : '{conso_col_guess}'")
            st.warning("‚ö†Ô∏è Veuillez s√©lectionner manuellement la colonne de date.")
        else:
            st.error("‚ùå **D√©tection automatique √©chou√©e** - S√©lection manuelle requise")
            
        # Affichage des informations sur le fichier
        st.markdown(f"""
        <div class="metrics-card">
            <h4>üìä Informations sur le fichier</h4>
            <ul>
                <li><strong>Nombre de lignes :</strong> {len(df)}</li>
                <li><strong>Nombre de colonnes :</strong> {len(df.columns)}</li>
                <li><strong>Colonnes disponibles :</strong> {', '.join(df.columns[:5])}{'...' if len(df.columns) > 5 else ''}</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)
        
    except Exception as e:
        st.error(f"‚ùå **Erreur lors du chargement** : {str(e)}")
        df = None
        date_col_guess = None
        conso_col_guess = None
else:
    df = None
    date_col_guess = None
    conso_col_guess = None

# SIDEBAR - S√âLECTION DES DONN√âES AVEC CONTR√îLES ADAPTATIFS
st.sidebar.header("üîç Configuration de l'analyse")

# S√©lection des colonnes
date_col = st.sidebar.selectbox(
    "üìÖ Colonne de date", 
    df.columns if df is not None else [""],
    index=list(df.columns).index(date_col_guess) if df is not None and date_col_guess in df.columns else 0
)

conso_col = st.sidebar.selectbox(
    "‚ö° Colonne de consommation", 
    df.columns if df is not None else [""],
    index=list(df.columns).index(conso_col_guess) if df is not None and conso_col_guess in df.columns else 0
)

# VALIDATION PR√âLIMINAIRE DES DONN√âES
if df is not None and date_col and conso_col:
    # Conversion et validation de base
    try:
        if not pd.api.types.is_datetime64_any_dtype(df[date_col]):
            df[date_col] = pd.to_datetime(df[date_col])
        df = df.sort_values(by=date_col)
        
        # Validation de la qualit√© des donn√©es
        var_options = [col for col in df.columns if col not in [date_col, conso_col]]
        selected_vars = st.sidebar.multiselect("üìä Variables explicatives", var_options)
        
        if selected_vars:
            issues, warnings = validate_data_quality(df, date_col, conso_col, selected_vars)
            
            # Affichage des probl√®mes critiques
            if issues:
                st.sidebar.markdown("### üö® Probl√®mes d√©tect√©s")
                for issue in issues:
                    st.sidebar.error(issue)
            
            # Affichage des avertissements
            if warnings:
                st.sidebar.markdown("### ‚ö†Ô∏è Avertissements")
                for warning in warnings:
                    st.sidebar.warning(warning)
                    
            # V√©rification des limitations de variables
            if len(selected_vars) > 0:
                nb_obs = len(df)
                var_issues, var_warnings = check_variable_limits(nb_obs, len(selected_vars), "G√©n√©ral")
                
                if var_issues:
                    st.sidebar.markdown("### üö´ Limitations d√©pass√©es")
                    for issue in var_issues:
                        st.sidebar.error(issue)
                        
                if var_warnings:
                    for warning in var_warnings:
                        st.sidebar.warning(warning)
                        
                # Affichage du ratio actuel
                ratio = nb_obs / len(selected_vars) if len(selected_vars) > 0 else float('inf')
                if ratio < 10:
                    status_color = "#f44336" if ratio < 5 else "#ff9800"
                    status_text = "Critique" if ratio < 5 else "Attention"
                else:
                    status_color = "#4caf50"
                    status_text = "Bon"
                    
                st.sidebar.markdown(f"""
                <div style="background-color: rgba(109, 186, 188, 0.1); padding: 10px; border-radius: 5px; margin: 10px 0;">
                    <strong>üìä Ratio Observations/Variables:</strong><br>
                    <span style="color: {status_color}; font-weight: bold; font-size: 1.2em;">{ratio:.1f}:1</span>
                    <span style="color: {status_color};">({status_text})</span><br>
                    <small>Recommand√©: ‚â•10:1 | Minimum: ‚â•5:1</small>
                </div>
                """, unsafe_allow_html=True)
        
    except Exception as e:
        st.sidebar.error(f"‚ùå Erreur dans la validation des donn√©es : {str(e)}")

# S√âLECTION DE LA P√âRIODE AVEC MODE ADAPTATIF
if df is not None and date_col:
    # D√©termination du mode d'analyse
    nb_observations = len(df)
    use_train_test, mode_message = should_use_train_test_split(nb_observations)
    
    # Affichage du mode d'analyse
    if use_train_test:
        st.sidebar.markdown(f"""
        <div class="mode-indicator" style="background-color: rgba(150, 185, 29, 0.1); border-color: #96B91D;">
            <div class="mode-title">üöÄ Mode Validation Robuste</div>
            <p>Split train/test automatique (18/6 mois)<br>
            √âvaluation sur donn√©es non-vues</p>
        </div>
        """, unsafe_allow_html=True)
    else:
        st.sidebar.markdown(f"""
        <div class="mode-indicator" style="background-color: rgba(109, 186, 188, 0.1); border-color: #6DBABC;">
            <div class="mode-title">üìã Mode IPMVP Standard</div>
            <p>Analyse sur toutes les donn√©es<br>
            Protections anti-overfitting renforc√©es</p>
        </div>
        """, unsafe_allow_html=True)
    
    st.sidebar.info(mode_message)

# S√âLECTION DE P√âRIODE
period_choice = st.sidebar.radio(
    "üìÖ S√©lection de la p√©riode",
    ["Rechercher automatiquement la meilleure p√©riode de 12 mois", "S√©lectionner manuellement une p√©riode sp√©cifique"]
)

# S√©lection manuelle de p√©riode avec validation am√©lior√©e
if period_choice == "S√©lectionner manuellement une p√©riode sp√©cifique" and df is not None and date_col in df.columns:
    if pd.api.types.is_datetime64_any_dtype(df[date_col]):
        min_date = df[date_col].min().date()
        max_date = df[date_col].max().date()
        
        col1, col2 = st.sidebar.columns(2)
        with col1:
            start_date = st.date_input("üìÖ D√©but", 
                                     value=min_date,
                                     min_value=min_date, 
                                     max_value=max_date)
        with col2:
            default_end = min(max_date, (pd.to_datetime(start_date) + pd.DateOffset(months=11)).date())
            end_date = st.date_input("üìÖ Fin", 
                                   value=default_end,
                                   min_value=start_date, 
                                   max_value=max_date)
        
        # Calcul et validation de la p√©riode
        months_diff = (end_date.year - start_date.year) * 12 + (end_date.month - start_date.month) + 1
        
        # Validation de la p√©riode avec messages adaptatifs
        if months_diff < 12:
            st.sidebar.warning(f"‚ö†Ô∏è P√©riode courte: {months_diff} mois (recommand√©: ‚â•12)")
        elif months_diff == 12:
            st.sidebar.success(f"‚úÖ P√©riode IPMVP standard: {months_diff} mois")
        elif months_diff < 24:
            st.sidebar.info(f"‚ÑπÔ∏è P√©riode √©tendue: {months_diff} mois")
        else:
            st.sidebar.success(f"‚úÖ P√©riode robuste: {months_diff} mois (train/test possible)")

# CONFIGURATION DU MOD√àLE AVEC LIMITATIONS DYNAMIQUES
st.sidebar.markdown("---")
st.sidebar.subheader("üßÆ Configuration du mod√®le")

model_type = st.sidebar.selectbox(
    "Type de mod√®le de r√©gression",
    ["Automatique (score composite)", "Lin√©aire", "Ridge", "Lasso", "Polynomiale"],
    index=0,
    help="Mode automatique recommand√© : teste tous les mod√®les et s√©lectionne selon le score composite IPMVP"
)

# Limitations dynamiques selon les donn√©es
if df is not None and len(selected_vars) > 0:
    max_vars_safe = len(df) // 10
    max_vars_absolute = len(df) // 5
    
    if max_vars_safe < 1:
        st.sidebar.error("‚ùå Donn√©es insuffisantes pour l'analyse")
        max_features = st.sidebar.slider("üî¢ Nombre de variables", 1, 1, 1, disabled=True)
    else:
        max_recommended = min(4, max_vars_safe)
        max_absolute = min(4, max_vars_absolute)
        
        max_features = st.sidebar.slider(
            "üî¢ Nombre de variables √† tester", 
            1, 
            max_absolute, 
            min(2, max_recommended),
            help=f"Recommand√©: ‚â§{max_recommended} | Maximum absolu: {max_absolute}"
        )
        
        # Warning si au-dessus du seuil recommand√©
        if max_features > max_recommended:
            st.sidebar.warning(f"‚ö†Ô∏è Au-dessus du seuil recommand√© ({max_recommended})")
else:
    max_features = st.sidebar.slider("üî¢ Nombre de variables √† tester", 1, 4, 2)

# Param√®tres sp√©cifiques aux mod√®les avec validation
if model_type == "Ridge":
    alpha_ridge = st.sidebar.slider("Alpha (r√©gularisation Ridge)", 0.01, 10.0, 1.0, 0.01)
elif model_type == "Lasso":
    alpha_lasso = st.sidebar.slider("Alpha (r√©gularisation Lasso)", 0.01, 1.0, 0.1, 0.01)
elif model_type == "Polynomiale":
    # V√©rification des limitations pour polyn√¥me
    if df is not None and len(df) < 20:
        st.sidebar.error("‚ùå Mod√®le polynomial n√©cessite ‚â•20 observations")
        poly_degree = st.sidebar.slider("Degr√© du polyn√¥me", 2, 2, 2, disabled=True)
    else:
        poly_degree = st.sidebar.slider("Degr√© du polyn√¥me", 2, 3, 2)
        if df is not None and len(df) < 30:
            st.sidebar.warning("‚ö†Ô∏è Recommandation: ‚â•30 observations pour polyn√¥me stable")

# INFORMATIONS SUR LA CONFORMIT√â IPMVP ENRICHIES
st.sidebar.markdown("---")
st.sidebar.markdown(f"""
### ‚úÖ Crit√®res IPMVP Am√©lior√©s
{tooltip("Score Composite", "Le nouveau syst√®me √©value les mod√®les sur un score 0-100 points combinant performance statistique, conformit√© IPMVP et simplicit√©. Fini le tri par R¬≤ seul !")}

**üìä Crit√®res principaux :**
- **R¬≤ ‚â• 0.75** : Corr√©lation excellente
- **CV(RMSE) ‚â§ 15%** : Pr√©cision excellente  
- **|Biais| < 5%** : Ajustement √©quilibr√©

**üéØ Nouveaux crit√®res :**
- **Significativit√©** : |t| > 2 (p-value < 0.05)
- **Ratio obs/var** : ‚â•10:1 (protection overfitting)
- **Simplicit√©** : Moins de variables = meilleur score
""", unsafe_allow_html=True)

# INFORMATIONS SUR LES MOD√àLES AVEC AM√âLIORATIONS
st.sidebar.markdown(f"""
### üßÆ Mod√®les disponibles

**üîÑ Mode automatique (recommand√©)**
- Teste tous les types de mod√®les
- S√©lection par **score composite IPMVP**
- Ridge/Lasso retrouvent leur utilit√© !

**üìà Mod√®les individuels**
- {tooltip("Lin√©aire", "Mod√®le standard IPMVP. Relation lin√©aire simple et interpr√©table.")}
- {tooltip("Ridge", "R√©gularisation L2. R√©duit l'overfitting, garde toutes les variables.")}
- {tooltip("Lasso", "R√©gularisation L1. Peut √©liminer des variables non pertinentes.")}
- {tooltip("Polynomiale", "Relations non-lin√©aires. Attention au risque d'overfitting !")}
""", unsafe_allow_html=True)

# GESTION DU COMPTE UTILISATEUR DANS LA SIDEBAR
st.sidebar.markdown("---")
st.sidebar.header("üë§ Gestion du compte")
st.sidebar.markdown(f"**Connect√© :** {st.session_state['username']}")

# Panel d'administration pour les admins
if st.session_state.get('is_admin', False):
    st.sidebar.markdown("#### üîê Administration")
    if st.sidebar.button("üë• G√©rer les utilisateurs", use_container_width=True):
        st.session_state['show_admin'] = True
        st.rerun()

# Changement de mot de passe
st.sidebar.markdown("#### üîë S√©curit√©")
with st.sidebar.expander("Changer de mot de passe"):
    with st.form("change_password_form"):
        current_password = st.text_input("Mot de passe actuel", type="password")
        new_password = st.text_input("Nouveau mot de passe", type="password")
        confirm_password = st.text_input("Confirmer le mot de passe", type="password")
        submit_password = st.form_submit_button("üîÑ Modifier", use_container_width=True)
        
        if submit_password:
            if not check_credentials(st.session_state['username'], current_password):
                st.error("‚ùå Mot de passe actuel incorrect")
            elif new_password != confirm_password:
                st.error("‚ùå Les mots de passe ne correspondent pas")
            elif not new_password:
                st.error("‚ùå Le nouveau mot de passe ne peut pas √™tre vide")
            else:
                update_user(st.session_state['username'], new_password)
                st.success("‚úÖ Mot de passe modifi√© !")

# Bouton de d√©connexion
if st.sidebar.button("üö™ D√©connexion", key="sidebar_logout", use_container_width=True):
    for key in list(st.session_state.keys()):
        del st.session_state[key]
    st.rerun()

# PIED DE PAGE AM√âLIOR√â
st.markdown("---")
st.markdown("""
<div class="footer-credit">
    <p><strong>üìä Analyse IPMVP Am√©lior√©e v2.0</strong></p>
    <p>‚ú® <strong>Nouveaut√©s :</strong> D√©tection overfitting ‚Ä¢ Score composite ‚Ä¢ Train/Test split ‚Ä¢ Limitations s√©curit√©</p>
    <p>D√©velopp√© avec ‚ù§Ô∏è par <strong>Efficacit√© Energ√©tique, Carbone & RSE team</strong> ¬© 2025</p>
</div>
""", unsafe_allow_html=True)
# =============================================================================
# PARTIE 4 : CALCUL ET R√âSULTATS
# Algorithme de calcul principal avec train/test et affichage des r√©sultats
# =============================================================================

# LANCEMENT DU CALCUL PRINCIPAL AVEC AM√âLIORATIONS IPMVP
if df is not None and lancer_calcul and selected_vars:
    
    # V√©rifications pr√©liminaires
    if not date_col or not conso_col:
        st.error("‚ùå **Veuillez s√©lectionner les colonnes de date et de consommation**")
        st.stop()
    
    if not selected_vars:
        st.error("‚ùå **Veuillez s√©lectionner au moins une variable explicative**")
        st.stop()
    
    # Initialisation
    st.subheader("‚öôÔ∏è Analyse en cours...")

    all_models = []
    
    # Conversion et tri des donn√©es
    try:
        if not pd.api.types.is_datetime64_any_dtype(df[date_col]):
            df[date_col] = pd.to_datetime(df[date_col])
        df = df.sort_values(by=date_col)
    except Exception as e:
        st.error(f"‚ùå **Erreur conversion date** : {str(e)}")
        st.stop()
    
    # OPTION 1: RECHERCHE AUTOMATIQUE DE LA MEILLEURE P√âRIODE
    if period_choice == "Rechercher automatiquement la meilleure p√©riode de 12 mois":
        
        # G√©n√©ration des p√©riodes candidates
        date_ranges = []
        min_date = df[date_col].min()
        max_date = df[date_col].max()
        current_date = min_date
        
        while current_date + pd.DateOffset(months=11) <= max_date:
            end_date = current_date + pd.DateOffset(months=11)
            period_name = f"{current_date.strftime('%b %Y')} - {end_date.strftime('%b %Y')}"
            date_ranges.append((period_name, current_date, end_date))
            current_date = current_date + pd.DateOffset(months=1)
        
        if not date_ranges:
            st.error("‚ùå **Donn√©es insuffisantes** pour une analyse sur 12 mois")
            st.stop()
        
        # Barre de progression am√©lior√©e
        progress_container = st.container()
        with progress_container:
            progress_bar = st.progress(0)
            progress_text = st.empty()
            col1, col2, col3 = st.columns(3)
            with col1:
                current_period = st.empty()
            with col2:
                current_score = st.empty()
            with col3:
                best_so_far = st.empty()
        
        # Variables pour le meilleur mod√®le
        best_period_data = None
        best_period_model = None
        best_period_features = None
        best_period_metrics = None
        best_period_score = -1
        best_period_name = None
        
        # Analyse de chaque p√©riode
        for idx, (period_name, period_start, period_end) in enumerate(date_ranges):
            current_period.info(f"üìÖ **{period_name}**")
            progress_text.text(f"Analyse p√©riode {idx+1}/{len(date_ranges)}")
            
            # Filtrer les donn√©es
            period_df = df[(df[date_col] >= period_start) & (df[date_col] <= period_end)]
            
            if len(period_df) < 10:
                continue
            
            # D√©terminer le mode d'analyse pour cette p√©riode
            use_train_test, _ = should_use_train_test_split(len(period_df))
            
            # Pr√©paration des donn√©es
            X = period_df[selected_vars]
            y = period_df[conso_col]
            
            # Nettoyage des donn√©es
            if X.isnull().values.any() or np.isinf(X.values).any():
                continue
            if y.isnull().values.any() or np.isinf(y.values).any():
                continue
            
            X = X.apply(pd.to_numeric, errors='coerce').dropna()
            y = pd.to_numeric(y, errors='coerce').dropna()
            
            # Validation des limitations de s√©curit√©
            var_issues, _ = check_variable_limits(len(period_df), len(selected_vars), model_type)
            if var_issues:
                continue
            
            period_best_score = -1
            period_best_model = None
            
            # Test des combinaisons de variables
            for n in range(1, min(max_features + 1, len(selected_vars) + 1)):
                for combo in combinations(selected_vars, n):
                    X_subset = X[list(combo)]
                    
                    # Split train/test si applicable
                    if use_train_test and len(period_df) >= 24:
                        train_df, test_df, split_date = create_train_test_split(period_df, date_col)
                        X_train = train_df[list(combo)]
                        y_train = train_df[conso_col]
                        X_test = test_df[list(combo)]
                        y_test = test_df[conso_col]
                        
                        # Nettoyage train/test
                        X_train = X_train.apply(pd.to_numeric, errors='coerce').dropna()
                        y_train = pd.to_numeric(y_train, errors='coerce').dropna()
                        X_test = X_test.apply(pd.to_numeric, errors='coerce').dropna()
                        y_test = pd.to_numeric(y_test, errors='coerce').dropna()
                        
                        if len(X_train) < 5 or len(X_test) < 3:
                            continue
                    else:
                        X_train, y_train = X_subset, y
                        X_test, y_test = None, None
                    
                    # Types de mod√®les √† tester
                    if model_type == "Automatique (score composite)":
                        model_types_to_test = [
                            ("Lin√©aire", LinearRegression(), "R√©gression lin√©aire"),
                            ("Ridge", Ridge(alpha=1.0), "R√©gression Ridge (Œ±=1.0)"),
                            ("Lasso", Lasso(alpha=0.1), "R√©gression Lasso (Œ±=0.1)")
                        ]
                        
                        # Ajouter polyn√¥me seulement si s√©curis√©
                        if len(period_df) >= 20:
                            model_types_to_test.append((
                                "Polynomiale", 
                                Pipeline([('poly', PolynomialFeatures(degree=2)), ('linear', LinearRegression())]),
                                "R√©gression polynomiale (degr√© 2)"
                            ))
                    else:
                        # Mod√®le sp√©cifique s√©lectionn√©
                        if model_type == "Lin√©aire":
                            model_obj = LinearRegression()
                            model_name = "R√©gression lin√©aire"
                        elif model_type == "Ridge":
                            model_obj = Ridge(alpha=alpha_ridge)
                            model_name = f"R√©gression Ridge (Œ±={alpha_ridge})"
                        elif model_type == "Lasso":  
                            model_obj = Lasso(alpha=alpha_lasso)
                            model_name = f"R√©gression Lasso (Œ±={alpha_lasso})"
                        elif model_type == "Polynomiale":
                            if len(period_df) < 20:
                                continue  # Skip si pas assez d'observations
                            model_obj = Pipeline([('poly', PolynomialFeatures(degree=poly_degree)), ('linear', LinearRegression())])
                            model_name = f"R√©gression polynomiale (degr√© {poly_degree})"
                        
                        model_types_to_test = [(model_type, model_obj, model_name)]
                    
                    # Test de chaque type de mod√®le
                    for m_type, m_obj, m_name in model_types_to_test:
                        try:
                            # Entra√Ænement du mod√®le
                            m_obj.fit(X_train, y_train)
                            
                            # Pr√©dictions et m√©triques
                            if X_test is not None and y_test is not None:
                                # Mode train/test
                                y_pred_train = m_obj.predict(X_train)
                                y_pred_test = m_obj.predict(X_test)
                                
                                # M√©triques sur le test set (priorit√©)
                                r2_test = r2_score(y_test, y_pred_test)
                                rmse_test = math.sqrt(mean_squared_error(y_test, y_pred_test))
                                cv_rmse_test = rmse_test / np.mean(y_test) if np.mean(y_test) != 0 else float('inf')
                                bias_test = np.mean(y_pred_test - y_test) / np.mean(y_test) * 100
                                
                                # M√©triques sur le train set
                                r2_train = r2_score(y_train, y_pred_train)
                                rmse_train = math.sqrt(mean_squared_error(y_train, y_pred_train))
                                cv_rmse_train = rmse_train / np.mean(y_train) if np.mean(y_train) != 0 else float('inf')
                                bias_train = np.mean(y_pred_train - y_train) / np.mean(y_train) * 100
                                
                                # Utiliser les m√©triques de test pour l'√©valuation
                                r2, cv_rmse, bias = r2_test, cv_rmse_test, bias_test
                                mae = mean_absolute_error(y_test, y_pred_test)
                                
                                # D√©tection d'overfitting par comparaison train/test
                                overfitting_detected = False
                                if abs(r2_train - r2_test) > 0.2:  # √âcart R¬≤ > 20%
                                    overfitting_detected = True
                                if cv_rmse_test > cv_rmse_train * 1.5:  # CV(RMSE) test >> train
                                    overfitting_detected = True
                                    
                            else:
                                # Mode standard (toutes les donn√©es)
                                y_pred = m_obj.predict(X_train)
                                r2 = r2_score(y_train, y_pred)
                                
                                # Calcul RMSE corrig√© selon IPMVP
                                n = len(y_train)
                                p = len(combo)
                                ssr = np.sum((y_train - y_pred) ** 2)
                                df_res = n - p - 1 if (n - p - 1) > 0 else 1
                                rmse = math.sqrt(ssr / df_res)
                                
                                cv_rmse = rmse / np.mean(y_train) if np.mean(y_train) != 0 else float('inf')
                                bias = np.mean(y_pred - y_train) / np.mean(y_train) * 100
                                mae = mean_absolute_error(y_train, y_pred)
                                
                                overfitting_detected = False
                                r2_train = r2_test = r2
                                cv_rmse_train = cv_rmse_test = cv_rmse
                                bias_train = bias_test = bias
                            
                            # D√©tection d'overfitting intelligent
                            model_info_temp = {
                                'r2': r2,
                                'cv_rmse': cv_rmse,
                                'bias': bias,
                                'features': list(combo),
                                'model_type': m_type
                            }
                            
                            is_overfitted, warning_msg, severity = detect_overfitting_intelligent(model_info_temp, len(period_df))
                            
                            # Rejeter si overfitting d√©tect√©
                            if is_overfitted and severity == "error":
                                continue
                            
                            # R√©cup√©ration des coefficients
                            if m_type in ["Lin√©aire", "Ridge", "Lasso"]:
                                coefs = {feature: coef for feature, coef in zip(combo, m_obj.coef_)}
                                intercept = m_obj.intercept_
                            elif m_type == "Polynomiale":
                                linear_model = m_obj.named_steps['linear']
                                poly = m_obj.named_steps['poly']
                                feature_names = poly.get_feature_names_out(input_features=combo)
                                coefs = {name: coef for name, coef in zip(feature_names, linear_model.coef_)}
                                intercept = linear_model.intercept_
                            
                            # Calcul des valeurs t
                            t_stats = calculate_t_stats(X_train, y_train, m_obj, coefs) if m_type in ["Lin√©aire", "Ridge", "Lasso"] else {feature: None for feature in combo}
                            
                            # √âvaluation conformit√© IPMVP
                            conformite, classe = evaluer_conformite(r2, cv_rmse, bias)
                            
                            # Cr√©ation du mod√®le info
                            model_info = {
                                'features': list(combo),
                                'r2': r2,
                                'rmse': rmse,
                                'cv_rmse': cv_rmse,
                                'mae': mae,
                                'bias': bias,
                                'coefficients': coefs,
                                'intercept': intercept,
                                'conformite': conformite,
                                'classe': classe,
                                'model_type': m_type,
                                'model_name': m_name,
                                'period': period_name,
                                't_stats': t_stats,
                                'overfitting_warning': warning_msg if not is_overfitted else "",
                                'overfitting_severity': severity if not is_overfitted else ""
                            }
                            
                            # Ajouter m√©triques train/test si disponibles
                            if X_test is not None:
                                model_info.update({
                                    'train_r2': r2_train,
                                    'test_r2': r2_test,
                                    'train_cv_rmse': cv_rmse_train,
                                    'test_cv_rmse': cv_rmse_test,
                                    'train_bias': bias_train,
                                    'test_bias': bias_test,
                                    'overfitting_train_test': overfitting_detected,
                                    'mode': 'train_test'
                                })
                            else:
                                model_info['mode'] = 'standard'
                            
                            # Calcul du score composite IPMVP
                            ipmvp_score = calculate_ipmvp_score(model_info, len(period_df))
                            model_info['ipmvp_score'] = ipmvp_score
                            
                            all_models.append(model_info)
                            
                            # Mise √† jour du meilleur mod√®le selon le score composite
                            if ipmvp_score > period_best_score:
                                period_best_score = ipmvp_score
                                period_best_model = model_info
                            
                            # Mise √† jour affichage en temps r√©el
                            current_score.metric("Score actuel", f"{ipmvp_score:.1f}/100")
                            
                        except Exception as e:
                            continue
            
            # Mise √† jour du meilleur mod√®le global
            if period_best_model and period_best_score > best_period_score:
                best_period_score = period_best_score
                best_period_data = period_df
                best_period_model = period_best_model
                best_period_features = period_best_model['features']
                best_period_metrics = period_best_model
                best_period_name = period_name
                
                best_so_far.metric("Meilleur score", f"{best_period_score:.1f}/100")
            
            # Mise √† jour de la barre de progression
            progress_bar.progress((idx + 1) / len(date_ranges))
        
        # Nettoyage de l'affichage de progression
        progress_container.empty()
        
        if best_period_data is not None:
            st.success(f"‚úÖ **Meilleure p√©riode trouv√©e** : {best_period_name}")
            
            # Warning si moins de 12 mois
            if len(best_period_data) < 12:
                st.warning(f"‚ö†Ô∏è **Attention :** Seulement {len(best_period_data)} observations disponibles. L'IPMVP recommande au minimum 12 mois de donn√©es pour une baseline fiable.")
            
            # Affichage du score composite
            col1, col2, col3 = st.columns(3)
            with col1:
                st.markdown(f"""
                <div class="score-card">
                    <div class="score-value">{best_period_score:.1f}</div>
                    <div class="score-label">Score IPMVP</div>
                </div>
                """, unsafe_allow_html=True)
            with col2:
                st.info(f"üìÖ **P√©riode** : {best_period_name}")
            with col3:
                st.info(f"üìä **Points de donn√©es** : {len(best_period_data)}")
            
            # Utiliser les meilleurs r√©sultats
            df_filtered = best_period_data
            best_model_obj = None  # √Ä reconstruire si n√©cessaire
            best_features = best_period_features
            best_metrics = best_period_metrics
            
        else:
            st.error("‚ùå **Aucun mod√®le valide trouv√©** sur les p√©riodes analys√©es")
            st.stop()
    
    # OPTION 2: P√âRIODE SP√âCIFIQUE S√âLECTIONN√âE
    else:
        # Filtrage selon la p√©riode s√©lectionn√©e
        df_filtered = df[(df[date_col].dt.date >= start_date) & (df[date_col].dt.date <= end_date)]
        
        st.info(f"üìä **Analyse sur p√©riode s√©lectionn√©e** : {start_date.strftime('%d/%m/%Y')} - {end_date.strftime('%d/%m/%Y')}")
        
        # Warning si moins de 12 mois
        if len(df_filtered) < 12:
            st.warning(f"‚ö†Ô∏è **Attention :** Seulement {len(df_filtered)} observations disponibles. L'IPMVP recommande au minimum 12 mois de donn√©es pour une baseline fiable.")
        
        # V√©rification donn√©es suffisantes
        if len(df_filtered) < 10:
            st.error("‚ùå **Donn√©es insuffisantes** pour l'analyse (minimum 10 points)")
            st.stop()
        
        # D√©termination du mode d'analyse
        use_train_test, mode_message = should_use_train_test_split(len(df_filtered))
        st.info(mode_message)
        
        # Pr√©paration des donn√©es
        X = df_filtered[selected_vars]
        y = df_filtered[conso_col]
        
        # Nettoyage des donn√©es
        if X.isnull().values.any() or np.isinf(X.values).any():
            st.error("‚ùå **Variables explicatives** contiennent des valeurs manquantes")
            st.stop()
        
        if y.isnull().values.any() or np.isinf(y.values).any():
            st.error("‚ùå **Colonne consommation** contient des valeurs manquantes")
            st.stop()
        
        X = X.apply(pd.to_numeric, errors='coerce').dropna()
        y = pd.to_numeric(y, errors='coerce').dropna()
        
        # Variables pour le meilleur mod√®le
        best_model_obj = None
        best_score = -1
        best_features = []
        best_metrics = {}
        
        # Barre de progression pour l'analyse
        total_combinations = sum(len(list(combinations(selected_vars, n))) for n in range(1, max_features + 1))
        progress_bar = st.progress(0)
        progress_counter = 0
        
        # Test des combinaisons de variables
        for n in range(1, min(max_features + 1, len(selected_vars) + 1)):
            for combo in combinations(selected_vars, n):
                progress_counter += 1
                progress_bar.progress(progress_counter / total_combinations)
                
                X_subset = X[list(combo)]
                
                # Split train/test si applicable
                if use_train_test and len(df_filtered) >= 24:
                    train_df, test_df, split_date = create_train_test_split(df_filtered, date_col)
                    X_train = train_df[list(combo)]
                    y_train = train_df[conso_col]
                    X_test = test_df[list(combo)]
                    y_test = test_df[conso_col]
                    
                    # Nettoyage train/test
                    X_train = X_train.apply(pd.to_numeric, errors='coerce').dropna()
                    y_train = pd.to_numeric(y_train, errors='coerce').dropna()
                    X_test = X_test.apply(pd.to_numeric, errors='coerce').dropna()
                    y_test = pd.to_numeric(y_test, errors='coerce').dropna()
                    
                    if len(X_train) < 5 or len(X_test) < 3:
                        continue
                else:
                    X_train, y_train = X_subset, y
                    X_test, y_test = None, None
                
                # Types de mod√®les √† tester (m√™me logique que pr√©c√©demment)
                if model_type == "Automatique (score composite)":
                    model_types_to_test = [
                        ("Lin√©aire", LinearRegression(), "R√©gression lin√©aire"),
                        ("Ridge", Ridge(alpha=1.0), "R√©gression Ridge (Œ±=1.0)"),
                        ("Lasso", Lasso(alpha=0.1), "R√©gression Lasso (Œ±=0.1)")
                    ]
                    
                    if len(df_filtered) >= 20:
                        model_types_to_test.append((
                            "Polynomiale", 
                            Pipeline([('poly', PolynomialFeatures(degree=2)), ('linear', LinearRegression())]),
                            "R√©gression polynomiale (degr√© 2)"
                        ))
                else:
                    # M√™me logique que pr√©c√©demment pour les mod√®les sp√©cifiques
                    if model_type == "Lin√©aire":
                        model_obj = LinearRegression()
                        model_name = "R√©gression lin√©aire"
                    elif model_type == "Ridge":
                        model_obj = Ridge(alpha=alpha_ridge)
                        model_name = f"R√©gression Ridge (Œ±={alpha_ridge})"
                    elif model_type == "Lasso":
                        model_obj = Lasso(alpha=alpha_lasso)
                        model_name = f"R√©gression Lasso (Œ±={alpha_lasso})"
                    elif model_type == "Polynomiale":
                        if len(df_filtered) < 20:
                            continue
                        model_obj = Pipeline([('poly', PolynomialFeatures(degree=poly_degree)), ('linear', LinearRegression())])
                        model_name = f"R√©gression polynomiale (degr√© {poly_degree})"
                    
                    model_types_to_test = [(model_type, model_obj, model_name)]
                
                # Test de chaque type de mod√®le (m√™me logique que l'analyse par p√©riode)
                for m_type, m_obj, m_name in model_types_to_test:
                    try:
                        # Entra√Ænement et √©valuation (m√™me code que pr√©c√©demment)
                        m_obj.fit(X_train, y_train)
                        
                        # Calcul des m√©triques (m√™me logique)
                        if X_test is not None and y_test is not None:
                            # Mode train/test
                            y_pred_train = m_obj.predict(X_train)
                            y_pred_test = m_obj.predict(X_test)
                            
                            r2_test = r2_score(y_test, y_pred_test)
                            rmse_test = math.sqrt(mean_squared_error(y_test, y_pred_test))
                            cv_rmse_test = rmse_test / np.mean(y_test) if np.mean(y_test) != 0 else float('inf')
                            bias_test = np.mean(y_pred_test - y_test) / np.mean(y_test) * 100
                            
                            r2_train = r2_score(y_train, y_pred_train)
                            rmse_train = math.sqrt(mean_squared_error(y_train, y_pred_train))
                            cv_rmse_train = rmse_train / np.mean(y_train) if np.mean(y_train) != 0 else float('inf')
                            bias_train = np.mean(y_pred_train - y_train) / np.mean(y_train) * 100
                            
                            r2, cv_rmse, bias = r2_test, cv_rmse_test, bias_test
                            mae = mean_absolute_error(y_test, y_pred_test)
                            rmse = rmse_test
                            
                            overfitting_detected = False
                            if abs(r2_train - r2_test) > 0.2 or cv_rmse_test > cv_rmse_train * 1.5:
                                overfitting_detected = True
                                
                        else:
                            # Mode standard
                            y_pred = m_obj.predict(X_train)
                            r2 = r2_score(y_train, y_pred)
                            
                            n = len(y_train)
                            p = len(combo)
                            ssr = np.sum((y_train - y_pred) ** 2)
                            df_res = n - p - 1 if (n - p - 1) > 0 else 1
                            rmse = math.sqrt(ssr / df_res)
                            
                            cv_rmse = rmse / np.mean(y_train) if np.mean(y_train) != 0 else float('inf')
                            bias = np.mean(y_pred - y_train) / np.mean(y_train) * 100
                            mae = mean_absolute_error(y_train, y_pred)
                            
                            overfitting_detected = False
                            r2_train = r2_test = r2
                            cv_rmse_train = cv_rmse_test = cv_rmse
                            bias_train = bias_test = bias
                        
                        # D√©tection d'overfitting et rejet si n√©cessaire
                        model_info_temp = {
                            'r2': r2,
                            'cv_rmse': cv_rmse,
                            'bias': bias,
                            'features': list(combo),
                            'model_type': m_type
                        }
                        
                        is_overfitted, warning_msg, severity = detect_overfitting_intelligent(model_info_temp, len(df_filtered))
                        
                        if is_overfitted and severity == "error":
                            continue
                        
                        # R√©cup√©ration des coefficients (m√™me logique)
                        if m_type in ["Lin√©aire", "Ridge", "Lasso"]:
                            coefs = {feature: coef for feature, coef in zip(combo, m_obj.coef_)}
                            intercept = m_obj.intercept_
                        elif m_type == "Polynomiale":
                            linear_model = m_obj.named_steps['linear']
                            poly = m_obj.named_steps['poly']
                            feature_names = poly.get_feature_names_out(input_features=combo)
                            coefs = {name: coef for name, coef in zip(feature_names, linear_model.coef_)}
                            intercept = linear_model.intercept_
                        
                        # Calcul des valeurs t
                        t_stats = calculate_t_stats(X_train, y_train, m_obj, coefs) if m_type in ["Lin√©aire", "Ridge", "Lasso"] else {feature: None for feature in combo}
                        
                        # Conformit√© IPMVP
                        conformite, classe = evaluer_conformite(r2, cv_rmse, bias)
                        
                        # Cr√©ation du mod√®le info complet
                        model_info = {
                            'features': list(combo),
                            'r2': r2,
                            'rmse': rmse,
                            'cv_rmse': cv_rmse,
                            'mae': mae,
                            'bias': bias,
                            'coefficients': coefs,
                            'intercept': intercept,
                            'conformite': conformite,
                            'classe': classe,
                            'model_type': m_type,
                            'model_name': m_name,
                            'period': 'selected',
                            't_stats': t_stats,
                            'overfitting_warning': warning_msg if not is_overfitted else "",
                            'overfitting_severity': severity if not is_overfitted else ""
                        }
                        
                        # M√©triques train/test
                        if X_test is not None:
                            model_info.update({
                                'train_r2': r2_train,
                                'test_r2': r2_test,
                                'train_cv_rmse': cv_rmse_train,
                                'test_cv_rmse': cv_rmse_test,
                                'train_bias': bias_train,
                                'test_bias': bias_test,
                                'overfitting_train_test': overfitting_detected,
                                'mode': 'train_test'
                            })
                        else:
                            model_info['mode'] = 'standard'
                        
                        # Score composite IPMVP
                        ipmvp_score = calculate_ipmvp_score(model_info, len(df_filtered))
                        model_info['ipmvp_score'] = ipmvp_score
                        
                        all_models.append(model_info)
                        
                        # Mise √† jour du meilleur mod√®le
                        if ipmvp_score > best_score:
                            best_score = ipmvp_score
                            best_model_obj = m_obj
                            best_features = list(combo)
                            best_metrics = model_info
                        
                    except Exception as e:
                        continue
        
        progress_bar.empty()
    
    # TRI DES MOD√àLES PAR SCORE COMPOSITE (PAS PAR R¬≤ !)
    all_models.sort(key=lambda x: x['ipmvp_score'], reverse=True)

    # AFFICHAGE DES R√âSULTATS AVEC AM√âLIORATIONS
    if best_metrics:
        st.success("‚úÖ **Analyse termin√©e avec succ√®s !**")
        
        # SCORE COMPOSITE ET INFORMATIONS PRINCIPALES
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.markdown(f"""
            <div class="score-card">
                <div class="score-value">{best_metrics['ipmvp_score']:.1f}</div>
                <div class="score-label">Score IPMVP</div>
            </div>
            """, unsafe_allow_html=True)
        
        with col2:
            conformity_class = f"status-{best_metrics['classe']}" if best_metrics['classe'] != 'medium' else "status-warning"
            st.markdown(f"""
            <div class="metrics-card" style="text-align: center;">
                <h4>Conformit√© IPMVP</h4>
                <span class="status-badge {conformity_class}">{best_metrics['conformite']}</span>
            </div>
            """, unsafe_allow_html=True)
        
        with col3:
            mode_info = "üöÄ Train/Test" if best_metrics.get('mode') == 'train_test' else "üìã Standard"
            st.markdown(f"""
            <div class="metrics-card" style="text-align: center;">
                <h4>Mode d'analyse</h4>
                <p><strong>{mode_info}</strong></p>
            </div>
            """, unsafe_allow_html=True)
        
        with col4:
            st.markdown(f"""
            <div class="metrics-card" style="text-align: center;">
                <h4>Mod√®le s√©lectionn√©</h4>
                <span class="model-badge">{best_metrics['model_name']}</span>
            </div>
            """, unsafe_allow_html=True)
        
        # ALERTES OVERFITTING SI D√âTECT√âES
        if best_metrics.get('overfitting_warning'):
            severity = best_metrics.get('overfitting_severity', 'warning')
            alert_class = f"alert-{severity}" if severity in ['error', 'warning', 'info'] else "alert-warning"
            st.markdown(f"""
            <div class="alert-card {alert_class}">
                <strong>‚ö†Ô∏è Attention :</strong> {best_metrics['overfitting_warning']}
            </div>
            """, unsafe_allow_html=True)
        
        # M√âTRIQUES PRINCIPALES AVEC COMPARAISON TRAIN/TEST
        st.subheader("üìä M√©triques d√©taill√©es")
        
        if best_metrics.get('mode') == 'train_test':
            # Affichage train/test c√¥te √† c√¥te
            st.markdown("""
            <div class="comparison-grid">
                <div class="train-card">
                    <h4>üéØ Entra√Ænement (18 mois)</h4>
                </div>
                <div class="test-card">
                    <h4>üß™ Test (6 mois)</h4>
                </div>
            </div>
            """, unsafe_allow_html=True)
            
            col1, col2 = st.columns(2)
            
            with col1:
                train_metrics = f"""
                <table class="stats-table">
                    <tr><th>M√©trique</th><th>Valeur Train</th></tr>
                    <tr><td>{tooltip("R¬≤", "Coefficient de d√©termination sur les donn√©es d'entra√Ænement")}</td><td>{best_metrics.get('train_r2', 0):.4f}</td></tr>
                    <tr><td>{tooltip("CV(RMSE)", "Coefficient de variation du RMSE sur l'entra√Ænement")}</td><td>{best_metrics.get('train_cv_rmse', 0):.4f}</td></tr>
                    <tr><td>{tooltip("Biais (%)", "Erreur syst√©matique en pourcentage sur l'entra√Ænement")}</td><td>{best_metrics.get('train_bias', 0):.2f}</td></tr>
                </table>
                """
                st.markdown(train_metrics, unsafe_allow_html=True)
            
            with col2:
                test_metrics = f"""
                <table class="stats-table">
                    <tr><th>M√©trique</th><th>Valeur Test</th></tr>
                    <tr><td>{tooltip("R¬≤", "Coefficient de d√©termination sur les donn√©es de test (validation)")}</td><td>{best_metrics['r2']:.4f}</td></tr>
                    <tr><td>{tooltip("CV(RMSE)", "Coefficient de variation du RMSE sur le test")}</td><td>{best_metrics['cv_rmse']:.4f}</td></tr>
                    <tr><td>{tooltip("Biais (%)", "Erreur syst√©matique en pourcentage sur le test")}</td><td>{best_metrics['bias']:.2f}</td></tr>
                </table>
                """
                st.markdown(test_metrics, unsafe_allow_html=True)
            
            # Analyse des √©carts train/test
            r2_gap = abs(best_metrics.get('train_r2', 0) - best_metrics['r2'])
            cv_gap = abs(best_metrics.get('train_cv_rmse', 0) - best_metrics['cv_rmse'])
            
            if best_metrics.get('overfitting_train_test'):
                st.warning(f"‚ö†Ô∏è **√âcart train/test d√©tect√©** : R¬≤ gap = {r2_gap:.3f}, CV(RMSE) gap = {cv_gap:.3f}")
            else:
                st.info(f"‚úÖ **Bonne stabilit√© train/test** : R¬≤ gap = {r2_gap:.3f}, CV(RMSE) gap = {cv_gap:.3f}")
        
        else:
            # Affichage standard
            col1, col2 = st.columns(2)
            
            with col1:
                standard_metrics = f"""
                <table class="stats-table">
                    <tr><th>M√©trique</th><th>Valeur</th></tr>
                    <tr><td>{tooltip("R¬≤", "Coefficient de d√©termination : proportion de variance expliqu√©e par le mod√®le")}</td><td>{best_metrics['r2']:.4f}</td></tr>
                    <tr><td>{tooltip("RMSE", "Root Mean Square Error : √©cart-type des r√©sidus")}</td><td>{best_metrics['rmse']:.4f}</td></tr>
                    <tr><td>{tooltip("CV(RMSE)", "Coefficient de variation du RMSE en pourcentage de la moyenne")}</td><td>{best_metrics['cv_rmse']:.4f}</td></tr>
                    <tr><td>{tooltip("MAE", "Mean Absolute Error : erreur absolue moyenne")}</td><td>{best_metrics['mae']:.4f}</td></tr>
                    <tr><td>{tooltip("Biais (%)", "Erreur syst√©matique du mod√®le en pourcentage")}</td><td>{best_metrics['bias']:.2f}</td></tr>
                </table>
                """
                st.markdown(standard_metrics, unsafe_allow_html=True)
            
            with col2:
                # Informations sur le mod√®le
                model_info_html = f"""
                <div class="metrics-card">
                    <h4>üîç Informations du mod√®le</h4>
                    <p><strong>Variables utilis√©es :</strong> {', '.join(best_features)}</p>
                    <p><strong>Nombre de variables :</strong> {len(best_features)}</p>
                    <p><strong>Observations :</strong> {len(df_filtered)}</p>
                    <p><strong>Ratio obs/var :</strong> {len(df_filtered)/len(best_features):.1f}:1</p>
                </div>
                """
                st.markdown(model_info_html, unsafe_allow_html=True)
        
        # √âQUATION DU MOD√àLE
        st.subheader("üìù √âquation d'ajustement")
        
        if best_metrics['model_type'] in ["Lin√©aire", "Ridge", "Lasso"]:
            equation = format_equation(best_metrics['intercept'], 
                                     {feature: best_metrics['coefficients'][feature] for feature in best_features})
        elif best_metrics['model_type'] == "Polynomiale":
            equation = format_equation(best_metrics['intercept'], best_metrics['coefficients'])
        
        st.markdown(f"""
        <div class="equation-box">
            <h4>üßÆ √âquation math√©matique :</h4>
            <p style="font-size: 16px; font-weight: bold;">{equation}</p>
        </div>
        """, unsafe_allow_html=True)
        
        # VALEURS T DE STUDENT POUR MOD√àLES LIN√âAIRES
        if 't_stats' in best_metrics and best_metrics['model_type'] in ["Lin√©aire", "Ridge", "Lasso"]:
            st.subheader("üìà Analyse de significativit√© statistique")
            
            # Construction du tableau de significativit√© avec st.dataframe (natif Streamlit)
            st.subheader("üìà Analyse de significativit√© statistique")
            
            significant_count = 0
            total_count = 0
            sig_data = []
            
            for feature in best_features:
                coef = best_metrics['coefficients'][feature]
                
                if feature in best_metrics['t_stats'] and best_metrics['t_stats'][feature] is not None:
                    t_stat = best_metrics['t_stats'][feature]
                    
                    if isinstance(t_stat, dict):
                        t_value = t_stat.get('t_value', 0)
                        p_value = t_stat.get('p_value', 1)
                        significant = t_stat.get('significant', False)
                    else:
                        t_value = t_stat
                        p_value = "N/A"
                        significant = abs(t_value) > 2
                    
                    total_count += 1
                    if significant:
                        significant_count += 1
                    
                    sig_data.append({
                        "Variable": feature,
                        "Coefficient": round(coef, 4),
                        "Valeur t": round(t_value, 3) if isinstance(t_value, (int, float)) else t_value,
                        "p-value": round(p_value, 4) if isinstance(p_value, (int, float)) else p_value,
                        "Significatif": "‚úÖ Oui" if significant else "‚ùå Non"
                    })
                else:
                    sig_data.append({
                        "Variable": feature,
                        "Coefficient": round(coef, 4),
                        "Valeur t": "N/A",
                        "p-value": "N/A",
                        "Significatif": "N/A"
                    })
            
            # Affichage avec st.dataframe (natif, toujours fonctionnel)
            st.dataframe(
                sig_data,
                use_container_width=True,
                hide_index=True,
                column_config={
                    "Variable": st.column_config.TextColumn("Variable", help="Variable explicative"),
                    "Coefficient": st.column_config.NumberColumn("Coefficient", help="Coefficient de r√©gression", format="%.4f"),
                    "Valeur t": st.column_config.TextColumn("Valeur t", help="Statistique t de Student"),
                    "p-value": st.column_config.TextColumn("p-value", help="Probabilit√© associ√©e"),
                    "Significatif": st.column_config.TextColumn("Significatif", help="Significatif si |t| > 2 (p < 0.05)")
                }
            )
            
            # R√©sum√© de la significativit√©
            if total_count > 0:
                sig_percentage = (significant_count / total_count) * 100
                if sig_percentage >= 100:
                    st.success(f"‚úÖ **Excellente significativit√©** : {significant_count}/{total_count} variables significatives ({sig_percentage:.0f}%)")
                elif sig_percentage >= 70:
                    st.info(f"‚úÖ **Bonne significativit√©** : {significant_count}/{total_count} variables significatives ({sig_percentage:.0f}%)")
                else:
                    st.warning(f"‚ö†Ô∏è **Significativit√© limit√©e** : {significant_count}/{total_count} variables significatives ({sig_percentage:.0f}%)")
        
        # VISUALISATIONS AM√âLIOR√âES
        st.subheader("üìä Visualisations")
        
        # Pr√©paration des donn√©es pour les graphiques
        if best_metrics.get('mode') == 'train_test':
            # Reconstituer les pr√©dictions train/test
            train_df, test_df, split_date = create_train_test_split(df_filtered, date_col)
            
            # Reconstruire le mod√®le pour les pr√©dictions
            if best_metrics['model_type'] == "Lin√©aire":
                model_for_viz = LinearRegression()
            elif best_metrics['model_type'] == "Ridge":
                model_for_viz = Ridge(alpha=1.0)
            elif best_metrics['model_type'] == "Lasso":
                model_for_viz = Lasso(alpha=0.1)
            elif best_metrics['model_type'] == "Polynomiale":
                model_for_viz = Pipeline([('poly', PolynomialFeatures(degree=2)), ('linear', LinearRegression())])
            
            X_train = train_df[best_features]
            y_train = train_df[conso_col]
            X_test = test_df[best_features]
            y_test = test_df[conso_col]
            
            model_for_viz.fit(X_train, y_train)
            y_pred_train = model_for_viz.predict(X_train)
            y_pred_test = model_for_viz.predict(X_test)
            
            # Concat√©nation pour l'affichage
            X_all = pd.concat([X_train, X_test])
            y_all = pd.concat([y_train, y_test])
            y_pred_all = np.concatenate([y_pred_train, y_pred_test])
            
            # Marqueurs pour train/test
            train_indices = range(len(y_train))
            test_indices = range(len(y_train), len(y_train) + len(y_test))
            
        else:
            # Mode standard
            X_all = df_filtered[best_features]
            y_all = df_filtered[conso_col]
            
            # Reconstruction du mod√®le
            if best_metrics['model_type'] == "Lin√©aire":
                model_for_viz = LinearRegression()
            elif best_metrics['model_type'] == "Ridge":
                model_for_viz = Ridge(alpha=1.0)
            elif best_metrics['model_type'] == "Lasso":
                model_for_viz = Lasso(alpha=0.1)
            elif best_metrics['model_type'] == "Polynomiale":
                model_for_viz = Pipeline([('poly', PolynomialFeatures(degree=2)), ('linear', LinearRegression())])
            
            model_for_viz.fit(X_all, y_all)
            y_pred_all = model_for_viz.predict(X_all)
            
            train_indices = range(len(y_all))
            test_indices = []
        
        # Configuration matplotlib avec th√®me IPMVP
        plt.style.use('seaborn-v0_8-whitegrid')
        plt.rcParams.update({
            'axes.facecolor': '#F5F5F5',
            'figure.facecolor': '#E7DDD9',
            'axes.edgecolor': '#00485F',
            'axes.labelcolor': '#00485F',
            'axes.titlecolor': '#00485F',
            'xtick.color': '#0C1D2D',
            'ytick.color': '#0C1D2D',
            'grid.color': '#00485F',
            'grid.alpha': 0.1
        })
        
        # GRAPHIQUE 1: Comparaison temporelle avec distinction train/test
        fig, ax = plt.subplots(figsize=(14, 8))
        
        if best_metrics.get('mode') == 'train_test':
            # Affichage train
            ax.bar(train_indices, y_all.iloc[train_indices], color="#96B91D", alpha=0.7, label="Consommation mesur√©e (Train)", width=0.8)
            ax.plot(train_indices, y_pred_all[train_indices], color="#2E7D32", marker='o', linewidth=2.5, markersize=5, label="Consommation ajust√©e (Train)")
            
            # Affichage test
            ax.bar(test_indices, y_all.iloc[test_indices], color="#6DBABC", alpha=0.7, label="Consommation mesur√©e (Test)", width=0.8)
            ax.plot(test_indices, y_pred_all[test_indices], color="#00485F", marker='s', linewidth=2.5, markersize=5, label="Consommation ajust√©e (Test)")
            
            # Ligne de s√©paration
            if len(train_indices) > 0:
                ax.axvline(x=max(train_indices), color='red', linestyle='--', linewidth=2, alpha=0.7, label='S√©paration Train/Test')
            
            title_suffix = f" (Train: {len(train_indices)} pts, Test: {len(test_indices)} pts)"
        else:
            ax.bar(train_indices, y_all, color="#6DBABC", alpha=0.8, label="Consommation mesur√©e")
            ax.plot(train_indices, y_pred_all, color="#96B91D", marker='o', linewidth=2.5, markersize=4, label="Consommation ajust√©e")
            title_suffix = f" ({len(train_indices)} points)"
        
        ax.set_title(f"üìä Comparaison Consommation Mesur√©e vs Ajust√©e{title_suffix}", fontweight='bold', fontsize=16, pad=20)
        ax.set_xlabel("Observations", fontweight='bold', fontsize=12)
        ax.set_ylabel("Consommation", fontweight='bold', fontsize=12)
        ax.legend(frameon=True, facecolor="#E7DDD9", edgecolor="#00485F", fontsize=10)
        ax.grid(True, linestyle='--', alpha=0.3)
        
        # Annotations enrichies
        # Score IPMVP
        ax.annotate(f"Score IPMVP = {best_metrics['ipmvp_score']:.1f}/100", 
                   xy=(0.02, 0.98), xycoords='axes fraction',
                   fontsize=13, fontweight='bold', color='#00485F',
                   bbox=dict(boxstyle="round,pad=0.5", facecolor="#E7DDD9", edgecolor="#00485F", alpha=0.9),
                   verticalalignment='top')
        
        # R¬≤ et CV(RMSE)
        metrics_text = f"R¬≤ = {best_metrics['r2']:.3f} | CV(RMSE) = {best_metrics['cv_rmse']:.3f}"
        if best_metrics.get('mode') == 'train_test':
            metrics_text += "\n(Mesur√© sur Test Set)"
        ax.annotate(metrics_text, 
                   xy=(0.02, 0.88), xycoords='axes fraction',
                   fontsize=11, color='#00485F',
                   bbox=dict(boxstyle="round,pad=0.4", facecolor="#E7DDD9", edgecolor="#6DBABC", alpha=0.85),
                   verticalalignment='top')
        
        # Nombre total de valeurs
        ax.text(0.98, 0.02, f"Total: {len(y_all)} valeurs",
               transform=ax.transAxes,
               fontsize=10, color='#00485F',
               bbox=dict(boxstyle="round,pad=0.3", facecolor="#E7DDD9", edgecolor="#6DBABC", alpha=0.8),
               verticalalignment='bottom', horizontalalignment='right')
        
        st.pyplot(fig)
        
        # GRAPHIQUES 2 ET 3: Dispersion et r√©sidus c√¥te √† c√¥te
        col1, col2 = st.columns(2)
        
        with col1:
            # Graphique de dispersion
            fig2, ax2 = plt.subplots(figsize=(8, 8))
            
            if best_metrics.get('mode') == 'train_test':
                scatter_train = ax2.scatter(y_all.iloc[train_indices], y_pred_all[train_indices], 
                                          color="#96B91D", alpha=0.8, s=60, edgecolor='#2E7D32', linewidth=1, label="Train")
                scatter_test = ax2.scatter(y_all.iloc[test_indices], y_pred_all[test_indices], 
                                         color="#6DBABC", alpha=0.8, s=60, edgecolor='#00485F', linewidth=1, label="Test")
                ax2.legend()
            else:
                scatter = ax2.scatter(y_all, y_pred_all, color="#6DBABC", alpha=0.8, s=60, edgecolor='#00485F', linewidth=1)
            
            # Ligne de r√©f√©rence y=x
            min_val = min(min(y_all), min(y_pred_all))
            max_val = max(max(y_all), max(y_pred_all))
            ax2.plot([min_val, max_val], [min_val, max_val], '--', color='#00485F', linewidth=2, alpha=0.8, label="R√©f√©rence y=x")
            
            ax2.set_title("üìà Consommation Mesur√©e vs Pr√©dite", fontweight='bold', fontsize=14)
            ax2.set_xlabel("Consommation Mesur√©e", fontweight='bold')
            ax2.set_ylabel("Consommation Pr√©dite", fontweight='bold')
            ax2.grid(True, linestyle='--', alpha=0.3)
            
            # Annotation
            # Annotation enrichie
        if best_metrics.get('mode') == 'train_test':
            metrics_text = f"R¬≤ (Test) = {best_metrics['r2']:.4f}\nCV(RMSE) = {best_metrics['cv_rmse']:.3f}"
        else:
            metrics_text = f"R¬≤ = {best_metrics['r2']:.4f}\nCV(RMSE) = {best_metrics['cv_rmse']:.3f}"
        ax2.annotate(metrics_text, xy=(0.05, 0.95), xycoords='axes fraction',
                        fontsize=11, fontweight='bold', color='#00485F',
                        bbox=dict(boxstyle="round,pad=0.3", facecolor="#E7DDD9", edgecolor="#00485F", alpha=0.8),
                        verticalalignment='top')
            
        st.pyplot(fig2)
        
        with col2:
            # Analyse des r√©sidus
            residus = y_all - y_pred_all
            
            fig3, ax3 = plt.subplots(figsize=(8, 8))
            
            if best_metrics.get('mode') == 'train_test':
                ax3.scatter(train_indices, residus[train_indices], color="#96B91D", alpha=0.8, s=60, 
                           edgecolor='#2E7D32', linewidth=1, label="R√©sidus Train")
                ax3.scatter(test_indices, residus[test_indices], color="#6DBABC", alpha=0.8, s=60, 
                           edgecolor='#00485F', linewidth=1, label="R√©sidus Test")
                ax3.legend()
            else:
                ax3.scatter(range(len(residus)), residus, color="#96B91D", alpha=0.8, s=60, 
                           edgecolor='#2E7D32', linewidth=1)
            
            ax3.axhline(y=0, color='#00485F', linestyle='-', alpha=0.8, linewidth=2)
            ax3.set_title("üìâ Analyse des R√©sidus", fontweight='bold', fontsize=14)
            ax3.set_xlabel("Observations", fontweight='bold')
            ax3.set_ylabel("R√©sidus", fontweight='bold')
            ax3.grid(True, linestyle='--', alpha=0.3)
            
            # Annotation
            ax3.annotate(f"Biais = {best_metrics['bias']:.2f}%", xy=(0.05, 0.95), xycoords='axes fraction',
                        fontsize=12, fontweight='bold', color='#00485F',
                        bbox=dict(boxstyle="round,pad=0.3", facecolor="#E7DDD9", edgecolor="#00485F", alpha=0.8))
            
            st.pyplot(fig3)
        
        # TABLEAU DE CLASSEMENT DES MOD√àLES AVEC SCORE COMPOSITE
        st.subheader("üèÜ Classement des mod√®les (Score composite IPMVP)")
        
        if all_models:
            # Limiter √† 15 mod√®les pour la lisibilit√©
            models_to_show = all_models[:15]
            
            models_summary = []
            for i, model in enumerate(models_to_show):
                # Indicateur de mode
                mode_icon = "üöÄ" if model.get('mode') == 'train_test' else "üìã"
                
                # Classe de conformit√© pour le style
                conformity_class = f"conformity-{model['classe']}"
                
                model_row = {
                    "üèÜ": f"{i+1}",
                    "Score": f"**{model['ipmvp_score']:.1f}**/100",
                    "Mode": mode_icon,
                    "Type": model['model_name'][:20] + ("..." if len(model['model_name']) > 20 else ""),
                    "Variables": ", ".join(model['features'][:2]) + ("..." if len(model['features']) > 2 else ""),
                    "R¬≤": f"{model['r2']:.3f}",
                    "CV(RMSE)": f"{model['cv_rmse']:.3f}",
                    "Biais(%)": f"{model['bias']:.1f}",
                    "Conformit√©": model['conformite']
                }
                
                # Ajouter warning si overfitting
                if model.get('overfitting_warning'):
                    model_row["‚ö†Ô∏è"] = "‚ö†Ô∏è"
                else:
                    model_row["‚ö†Ô∏è"] = ""
                
                models_summary.append(model_row)
            
            # Affichage du tableau
            df_summary = pd.DataFrame(models_summary)
            st.dataframe(df_summary, use_container_width=True, hide_index=True)
            
            # Statistiques du classement
            excellent_count = sum(1 for m in models_to_show if m['conformite'] == 'Excellente')
            good_count = sum(1 for m in models_to_show if m['conformite'] in ['Bonne', 'Acceptable'])
            overfitting_count = sum(1 for m in models_to_show if m.get('overfitting_warning'))
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("üèÜ Total mod√®les", len(models_to_show))
            with col2:
                st.metric("‚úÖ Excellents", excellent_count)
            with col3:
                st.metric("‚úÖ Acceptables", good_count)
            with col4:
                st.metric("‚ö†Ô∏è Avec warnings", overfitting_count)
        
        # EXPLICATIONS ET RECOMMANDATIONS
        with st.expander("üìö Comprendre le nouveau syst√®me de scoring"):
            st.markdown("""
            ### üéØ Score Composite IPMVP (0-100 points)
            
            **üîÑ Changement majeur :** Fini le tri par R¬≤ seul ! Le nouveau syst√®me utilise un score composite qui √©value :
            
            #### üìä Score de base (60 points max)
            - **R¬≤ (30pts)** : Performance statistique, pond√©r√©e selon les seuils IPMVP
            - **CV(RMSE) (20pts)** : Pr√©cision du mod√®le (plus faible = mieux)
            - **Biais (10pts)** : √âquilibre du mod√®le (proche de 0 = mieux)
            
            #### üéÅ Bonus/Malus (40 points max)
            - **Simplicit√© (15pts)** : Moins de variables = mod√®le plus robuste
            - **Conformit√© IPMVP (15pts)** : Respect des crit√®res standard
            - **Significativit√© (10pts)** : Variables avec |t| > 2
            - **Malus overfitting** : -15 √† -30pts selon la s√©v√©rit√©
            - **Malus complexit√©** : -5pts pour polyn√¥me
            
            #### ‚úÖ Avantages du nouveau syst√®me
            - **Ridge/Lasso retrouvent leur utilit√©** : Pas p√©nalis√©s pour leur R¬≤ plus faible
            - **Fin des R¬≤ artificiels** : Mod√®les avec 99% de R¬≤ mais instables sont rejet√©s
            - **√âvaluation holistique** : Combine performance, robustesse et simplicit√©
            - **Conformit√© IPMVP renforc√©e** : Crit√®res standard int√©gr√©s au scoring
            """)
            
        with st.expander("üìö Interpr√©tation des r√©sultats"):
            st.markdown(f"""
            ### üîç Analyse de votre mod√®le
            
            **üèÜ Score obtenu :** {best_metrics['ipmvp_score']:.1f}/100
            - 90-100 : Excellent mod√®le, tr√®s robuste
            - 70-89 : Bon mod√®le, fiable pour IPMVP
            - 50-69 : Mod√®le acceptable, √† surveiller
            - <50 : Mod√®le insuffisant, r√©vision n√©cessaire
            
            **üìä Mode d'analyse :** {best_metrics.get('mode', 'standard').title()}
            {'- Validation sur donn√©es non-vues (train/test)' if best_metrics.get('mode') == 'train_test' else '- Analyse sur toutes les donn√©es disponibles'}
            {'- Plus robuste mais n√©cessite ‚â•24 mois' if best_metrics.get('mode') == 'train_test' else '- Standard IPMVP avec protections renforc√©es'}
            
            **üßÆ Type de mod√®le :** {best_metrics['model_name']}
            - Lin√©aire : Simple et interpr√©table
            - Ridge : R√©gularis√©, g√®re bien les corr√©lations
            - Lasso : S√©lection automatique de variables
            - Polynomiale : Relations non-lin√©aires, attention √† l'overfitting
            
            **‚ö° Variables utilis√©es :** {', '.join(best_features)}
            - Chaque variable doit avoir une justification physique
            - Privil√©gier les variables significatives (|t| > 2)
            - √âviter la redondance entre variables
            """, unsafe_allow_html=True)

        
        with st.expander("üìö Recommandations d'am√©lioration"):
            recommendations = []
            
            if best_metrics['ipmvp_score'] < 70:
                recommendations.append("üéØ **Score faible** : Envisager d'autres variables explicatives ou une p√©riode diff√©rente")
            
            if best_metrics['r2'] < 0.75:
                recommendations.append("üìä **R¬≤ insuffisant** : Le mod√®le explique moins de 75% de la variance (seuil IPMVP)")
            
            if best_metrics['cv_rmse'] > 0.15:
                recommendations.append("üéØ **Pr√©cision limit√©e** : CV(RMSE) > 15% (seuil IPMVP)")
            
            if abs(best_metrics['bias']) > 5:
                recommendations.append("‚öñÔ∏è **Biais √©lev√©** : Le mod√®le surestime ou sous-estime syst√©matiquement")
            
            if best_metrics.get('overfitting_warning'):
                recommendations.append("‚ö†Ô∏è **Risque d'overfitting** : " + best_metrics['overfitting_warning'])
            
            if best_metrics.get('mode') == 'standard' and len(df_filtered) >= 24:
                recommendations.append("üöÄ **Am√©lioration possible** : Vous avez assez de donn√©es pour le mode train/test (validation robuste)")
            
            # Analyse de la significativit√©
            if 't_stats' in best_metrics and best_metrics['model_type'] in ["Lin√©aire", "Ridge", "Lasso"]:
                non_significant = []
                for feature in best_features:
                    if feature in best_metrics['t_stats'] and best_metrics['t_stats'][feature] is not None:
                        t_stat = best_metrics['t_stats'][feature]
                        if isinstance(t_stat, dict):
                            significant = t_stat.get('significant', False)
                        else:
                            significant = abs(t_stat) > 2
                        
                        if not significant:
                            non_significant.append(feature)
                
                if non_significant:
                    recommendations.append(f"üìâ **Variables non significatives** : {', '.join(non_significant)} (envisager de les retirer)")
            
            if len(df_filtered) / len(best_features) < 10:
                recommendations.append("üìä **Ratio obs/variables faible** : Risque d'instabilit√©, consid√©rer moins de variables")
            
            if not recommendations:
                recommendations.append("‚úÖ **Excellent mod√®le** : Aucune am√©lioration majeure n√©cessaire !")
            
            for rec in recommendations:
                st.markdown(f"- {rec}")
        
        # R√âSUM√â EX√âCUTIF
        st.markdown("---")
        st.subheader("üìã R√©sum√© ex√©cutif")
        
        # D√©termination du statut global
        if best_metrics['ipmvp_score'] >= 80 and best_metrics['conformite'] == 'Excellente':
            status = "‚úÖ **MOD√àLE EXCELLENT**"
            status_color = "#4caf50"
            status_msg = "Mod√®le hautement fiable, conforme aux standards IPMVP les plus exigeants."
        elif best_metrics['ipmvp_score'] >= 60 and best_metrics['conformite'] in ['Excellente', 'Bonne']:
            status = "‚úÖ **MOD√àLE ACCEPTABLE**"
            status_color = "#2196f3"
            status_msg = "Mod√®le valide pour utilisation IPMVP avec quelques am√©liorations possibles."
        elif best_metrics['ipmvp_score'] >= 40:
            status = "‚ö†Ô∏è **MOD√àLE √Ä AM√âLIORER**"
            status_color = "#ff9800"
            status_msg = "Mod√®le pr√©sentant des limitations, r√©vision recommand√©e avant utilisation."
        else:
            status = "‚ùå **MOD√àLE INSUFFISANT**"
            status_color = "#f44336"
            status_msg = "Mod√®le non conforme aux standards IPMVP, r√©vision majeure n√©cessaire."
        
        # Affichage du r√©sum√© avec composants natifs Streamlit (plus fiable)
        st.markdown(f"### {status}")
        st.info(status_msg)
        
        # M√©triques en colonnes
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric(
                label="üèÜ Score IPMVP",
                value=f"{best_metrics['ipmvp_score']:.1f}/100",
                help="Score composite √©valuant performance, conformit√© IPMVP et simplicit√©"
            )
            st.metric(
                label="üìä R¬≤",
                value=f"{best_metrics['r2']:.3f}",
                help="Coefficient de d√©termination (‚â•0.75 pour excellente conformit√© IPMVP)"
            )
        
        with col2:
            st.metric(
                label="üéØ CV(RMSE)",
                value=f"{best_metrics['cv_rmse']:.3f}",
                help="Coefficient de variation RMSE (‚â§0.15 pour excellente conformit√© IPMVP)"
            )
            st.metric(
                label="‚öñÔ∏è Biais",
                value=f"{best_metrics['bias']:.1f}%",
                help="Erreur syst√©matique du mod√®le (|biais| < 5% recommand√©)"
            )
        
        with col3:
            st.metric(
                label="üßÆ Mod√®le",
                value=best_metrics['model_name'][:20],
                help="Type de r√©gression utilis√©"
            )
            st.metric(
                label="üìã Variables",
                value=f"{len(best_features)}",
                help=f"Variables: {', '.join(best_features)}"
            )
        
    else:
        st.error("‚ùå **Aucun mod√®le valide trouv√©**")
        st.markdown("""
        ### üîç Causes possibles :
        - **Donn√©es insuffisantes** : Moins de 10 observations
        - **Variables non pertinentes** : Aucune corr√©lation avec la consommation
        - **Overfitting d√©tect√©** : Tous les mod√®les rejet√©s pour R¬≤ suspect
        - **Limitations d√©pass√©es** : Trop de variables par rapport aux observations
        
        ### üí° Solutions :
        1. **V√©rifier les donn√©es** : Qualit√©, compl√©tude, coh√©rence
        2. **Revoir les variables** : Choisir des variables physiquement li√©es √† la consommation
        3. **Ajuster les param√®tres** : R√©duire le nombre de variables ou changer la p√©riode
        4. **Am√©liorer les donn√©es** : Ajouter plus d'observations si possible
        """)

elif df is not None and lancer_calcul and not selected_vars:
    st.warning("‚ö†Ô∏è **Veuillez s√©lectionner au moins une variable explicative** pour lancer l'analyse.")

elif lancer_calcul and df is None:
    st.warning("‚ö†Ô∏è **Veuillez d'abord importer un fichier Excel** pour lancer l'analyse.")

# MESSAGE INFORMATIF SI AUCUNE ACTION
elif df is None:
    st.info("""
    ### üöÄ Pour commencer votre analyse IPMVP :
    
    1. **üìÇ Importez votre fichier Excel** contenant :
       - Une colonne de dates (format date reconnu)
       - Une colonne de consommation √©nerg√©tique (valeurs num√©riques)
       - Des variables explicatives (DJU, temp√©rature, occupation, production, etc.)
    
    2. **üîç Configurez l'analyse** dans le panneau lat√©ral :
       - V√©rifiez la d√©tection automatique des colonnes
       - S√©lectionnez vos variables explicatives
       - Choisissez le mode d'analyse (automatique recommand√©)
    
    3. **üöÄ Lancez l'analyse** et d√©couvrez :
       - Le score composite IPMVP (0-100 points)
       - La d√©tection intelligente d'overfitting
       - La validation train/test si applicable
       - Les recommandations d'am√©lioration
    
    ### ‚ú® **Nouveaut√©s de cette version :**
    - **üõ°Ô∏è Protection anti-overfitting** : Fini les R¬≤ artificiels √† 99% !
    - **üéØ Score composite** : √âvaluation holistique rempla√ßant le tri par R¬≤ seul
    - **üöÄ Mode train/test** : Validation robuste si ‚â•24 mois de donn√©es
    - **‚ö†Ô∏è Limitations intelligentes** : Contr√¥le automatique du ratio observations/variables
    - **üìä M√©triques enrichies** : Significativit√© statistique, comparaisons train/test
    """)

# PIED DE PAGE FINAL
st.markdown("---")
st.markdown("""
<div class="footer-credit">
    <p><strong>üéâ Analyse IPMVP Am√©lior√©e v2.1 - Visualisations enrichies ! üéâ</strong></p>
    <p><strong>üîß Am√©liorations int√©gr√©es :</strong></p>
    <ul style="text-align: left; display: inline-block;">
        <li>‚úÖ D√©tection overfitting intelligente</li>
        <li>‚úÖ Score composite IPMVP (0-100 points)</li>
        <li>‚úÖ Mode train/test adaptatif</li>
        <li>‚úÖ Limitations s√©curit√© (r√®gle 10:1)</li>
        <li>‚úÖ M√©triques enrichies et visualisations am√©lior√©es</li>
        <li>‚úÖ Affichage d√©taill√© des intervalles train/test avec dates</li>
        <li>‚úÖ R¬≤ et CV(RMSE) visibles sur tous les graphiques</li>
        <li>‚úÖ Ridge/Lasso retrouvent leur utilit√©</li>
    </ul>
    <p>D√©velopp√© avec ‚ù§Ô∏è par <strong>Efficacit√© Energ√©tique, Carbone & RSE team</strong> ¬© 2025</p>
    <p><em>"Plus de R¬≤ √† 99% bidons, place aux mod√®les robustes !" üöÄ</em></p>
</div>
""", unsafe_allow_html=True)
